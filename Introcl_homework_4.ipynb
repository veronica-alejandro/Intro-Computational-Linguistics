{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 353C: Introduction to Computational Linguistics,  Fall 2022, Erk\n",
    "\n",
    "# Homework 4:  Naive Bayes, and Classification\n",
    "\n",
    "## Due: Wednesday October 12, 3pm right before class\n",
    "\n",
    "## Your name: Veronica Alejandro\n",
    "## Your EID: vaa678\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_4.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "* wsd_train.txt: training data\n",
    "* wsd_test.txt: test data\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Before you do anything else, please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine.\n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Word Sense Disambiguation by hand (40 points)\n",
    "\n",
    "Many words have different meanings, depending on their context. For example, in \n",
    "\n",
    "    They went to the *bank* to get money.\n",
    "    \n",
    "the word \"bank\" means a financial institution, but in \n",
    "\n",
    "    They sat on the *bank*, fishing.\n",
    "    \n",
    "the word \"bank\" means the slope at the side of a river.\n",
    "Some dictionaries list all the different senses that a word can take on. In computational linguistics, the WordNet dictionary is often used because it is available electronically. (In fact, NLTK has an interface to WordNet, but we are not using it for the current homework.)\n",
    "For example, here is the list of WordNet senses for the word \"mouse\": http://wordnetweb.princeton.edu/perl/webwn?c=7&sub=Change&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&i=-1&h=000000&s=mouse  It lists four noun senses, `mouse#1`, `mouse#2`, `mouse#3` and `mouse#4`, and two verb senses.\n",
    "\n",
    "This problem asks you to do some manual word sense annotation, and to comment on your observations. Please record your annotation in the box below. \n",
    "\n",
    "\n",
    "**Data.** Below, you find sentences from the SemCor corpus, all featuring the verb \"leave\". Look up the WordNet senses of the verb \"leave\" at http://wordnetweb.princeton.edu/perl/webwn?s=leave&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=1&o5=&o9=&o6=&o3=&o4=&h=000000 **using only the verb senses of the word**.\n",
    "\n",
    "For each of the sentences below, choose one best fitting Word-Net sense for the verb \"leave\" in that sentence, and record your choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sentences to annotate: Please record the sense number of your choice in this box, below each sentence.\n",
    "\n",
    "*(a) But questions with which committee members taunted bankers appearing as witnesses **left** little doubt that they will recommend passage of it .*\n",
    "\n",
    "Sense: Leave #6 -- make a possibility or provide opportunity for; permit to be attainable or cause to remain\n",
    "\n",
    "*(b) The departure of the Giants and the Dodgers to California **left** New York with only the Yankees .*\n",
    "\n",
    "Sense: Leave #1 -- go away from a place\n",
    "\n",
    "*(c) After the coach listed all the boy ’s faults , Hartweger said , “ Coach before I **leave** here , you ’ll get to like me ” .*\n",
    "\n",
    "Sense: Leave #8 -- remove oneself from an association with or participation in\n",
    "\n",
    "*(d) R. H. S. Crossman , M.P. , writing in The Manchester Guardian , states that departures from West Berlin are now running at the rate not of 700 , but of 1700 a week , and applications to **leave** have risen to 1900 a week .*\n",
    "\n",
    "Sense: Leave #5 -- move out of or depart from\n",
    "\n",
    "*(e) The house has been swept so clean that contemporary man has been **left** with no means , or at best with wholly inadequate means , for dealing with his experience of spirit .*\n",
    "\n",
    "Sense: Leave #3 -- act or be so as to become in a specified state\n",
    "\n",
    "*(f) A second and also good practice is to shear off the tops , **leaving** an inch high stub with just a leaf or two on each branch .*\n",
    "\n",
    "Sense: Leave #7 -- produce as a result or residue\n",
    "\n",
    "*(g) No doubt some experiences vanish so completely as to **leave** no trace on the sleeper ’s mind .*\n",
    "\n",
    "Sense: Leave #6 -- make a possibility or provide opportunity for; permit to be attainable or cause to remain\n",
    "\n",
    "*(h) He is a widower , his three children are dead , he has no one **left** on earth ; also he is a drunk , and has lost his job on that account .*\n",
    "\n",
    "Sense: Leave #12 -- be survived by after one's death\n",
    "\n",
    "*(i) Piepsam tries to stop him by force , receives a push in the chest from “ Life ” , and is **left** standing in impotent and growing rage , while a crowd begins to gather .*\n",
    "\n",
    "Sense: Leave #3 -- act or be so as to become in a specified state\n",
    "\n",
    "*(j) The audience **leaves** the play under a spell , It is the kind of spell which the exposure to spirit in its living active manifestation always evokes .*\n",
    "\n",
    "Sense: Leave #5 -- move out of or depart from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect on your annotation work \n",
    "\n",
    "Which sentences were hard to annotate, and why?\n",
    "\n",
    "Which pairs (or groups) of WordNet senses did you find hard to distinguish, and what criteria did you use to distinguish between them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found sentence (h) to difficult because I was not really sure how to describe that meaning of \"left.\" I thought none of the other definitions really fit and went with the one that made most sense since the man was a widow. I found it hard to distinguish between #1 and #5 because they are really similar, I took the latter to mean more like \"exit,\" which helped me distinguish them, although I'm not sure if it's corect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Naive Bayes: Computing the probability of each target class by hand (10 points)\n",
    "\n",
    "The file `wsd_train.txt` distributed with this homework contains word sense annotation data for the lemma feel. There is data for two Word- Net senses of feel:\n",
    "\n",
    "  * “experience”: to undergo an emotional sensation or be in a par- ticular state of mind. As in: “She felt happy”; “He felt regret”\n",
    "  * “think”: to come to believe on the basis of emotion, intuitions, or indefinite grounds. As in: “I feel that they like me”\n",
    "  \n",
    "Each line starts with a word that is the sense label, either `EXPERIENCE` or `THINK`. After that comes the sentence. Here is the first line as an example:\n",
    "\n",
    "    EXPERIENCE  I began to *feel* some guilt\n",
    "    \n",
    "This says that the sense label for this item is `EXPERIENCE`, and that this item contains the features I, began, to, some, guilt. The target word is encased in stars so you can omit it from feature counts.\n",
    "\n",
    "For this problem, please compute the overall probabilities of the two senses, THINK and EXPERIENCE, from the training data, that is, please compute $P(THINK)$ and $P(EXPERIENCE)$. Use Maximum Likelihood Estimation, that is, estimate the probabilities to be the same as relative frequencies in the training data. **For this problem, please do this by hand.** Show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 27 sentences, 15 of which are EXPERIENCE and the rest, 12 of them, are THINK. Thus, $P(EXPERIENCE)$ is $\\frac{15}{27}$ which means that $P(THINK) = \\frac{12}{27}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Naive Bayes: Computing the probability of some features by hand (10 points)\n",
    "\n",
    "The features appearing in the single test datapoint in `wsd_test.txt` are I,that,there,was,a,pain,in,my,leg. \n",
    "\n",
    "For each of these features, compute its conditional probability under each sense. That is, compute $P(I|THINK)$ and $P(I|EXPERIENCE)$, $P(that|THINK)$ and $P(that|EXPERIENCE)$, and so on for all the features in the test datapoint. **For this problem, please do the computation by hand.**\n",
    "\n",
    "To remind you how this works: While we cannot know, in the English language in general, how likely the\n",
    "word I is to appear given that we have an occurrence of feel in the THINK sense we can estimate the probabilities from the training data. For example, $P(I|THINK)$ would be estimated by concatenating all the THINK sentences into one long \"document\", and count how often \"I\" appears in it. Also compute how long this \"document\" is overall. Then the estimated probability is $\\frac{count_{THINK}(I)}{count_{THINK}(\\_)}$. You do not need to apply any smoothing.\n",
    "\n",
    "Again, please solve this problem by hand, and show your work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the THINK class, there are 6 occurences of \"I\" and 108 words total. Thus $P(I|THINK) = \\frac{6}{108}$\n",
    "- In the EXPERIENCE class, there are 9 occurences of \"I\" and 102 words total. Thus $P(I|EXPERIENCE) = \\frac{9}{102}$\n",
    "\n",
    "<br>\n",
    "\n",
    "- In the THINK class, there are 7 occurences of \"that.\" Thus, $P(that|THINK) = \\frac{7}{108}$\n",
    "- In the EXPERIENCE class, there are 2 occurrences of \"that.\" Thus $P(that|EXPERIENCE) = \\frac{2}{102}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- In the THINK class, there are 1 occurence of \"there.\" Thus $P(there|THINK) = \\frac{1}{108}$\n",
    "- In the EXPERIENCE class, there are 1 occurrence of \"there.\" Thus $P(there|EXPERIENCE) = \\frac{1}{102}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- In the THINK class, there are 2 occurences of \"was.\" Thus $P(was|THINK) = \\frac{2}{108}$\n",
    "- In the EXPERIENCE class, there are 1 occurrences of \"was.\" Thus $P(was|EXPERIENCE) = \\frac{1}{108}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- In the THINK class, there are 1 occurence of \"a.\" Thus $P(a|THINK) = \\frac{1}{108}$\n",
    "- In the EXPERIENCE class, there are 5 occurrences of \"a.\" Thus $P(was|EXPERIENCE) = \\frac{5}{102}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- In the THINK class, there are 1 occurence of \"pain.\" Thus $P(pain|THINK) = \\frac{1}{108}$\n",
    "- In the EXPERIENCE class, there are 4 occurrences of \"pain.\" Thus $P(pain|EXPERIENCE) = \\frac{4}{102}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- In the THINK class, there are 1 occurence of \"in.\" Thus $P(in|THINK) = \\frac{1}{108}$\n",
    "- In the EXPERIENCE class, there are 1 occurrence of \"in.\" Thus $P(in|EXPERIENCE) = \\frac{1}{102}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- In the THINK class, there are 1 occurences of \"my.\" Thus $P(my|THINK) = \\frac{1}{108}$\n",
    "- In the EXPERIENCE class, there are 4 occurrences of \"my.\" Thus $P(in|EXPERIENCE) = \\frac{4}{102}$\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- In the THINK class, there are 1 occurence of \"leg.\" Thus $P(leg|THINK) = \\frac{1}{108}$\n",
    "- In the EXPERIENCE class, there are 1 occurrences of \"leg.\" Thus $P(leg|EXPERIENCE) = \\frac{1}{102}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Naive Bayes: Computing the most likely class label for a datapoint by hand (10 points)\n",
    "\n",
    "For this problem, you will again use the data from problems 2 and 3, this time to determine the most likely sense for the test item. The test item is \n",
    "    I *felt* that there was a pain in my leg.\n",
    "    \n",
    "Remember that in Naive Bayes, the most likely sense $c$ for datapoint $d$ is the one for which\n",
    "    $P(c) (d|c)$\n",
    "    \n",
    "is largest. Per the \"naive\" assumption, we compute the probability of the datapoint given the class, $P(d|c)$, as follows: If $d$ contains features $f_1, \\ldots, f_n$, then $P(d|c) \\approx \\prod_{i=1}^n P(f_i|c)$. Here $\\prod_{i=1}^n P(f_i(c)$ is  $P(f_1|c)$ multiplied with $P(f_2|c)$ multiplied with... and multiplied with $P(f_n|c)$. \n",
    "\n",
    "You will need to re-use your probabilities $P(THINK)$ and $P(EXPERIENCE)$ from problem 2, and the feature probabilities from problem 3.\n",
    "\n",
    "You will need to compute this value $P(c) \\prod_f P(f|c)$ both for $c=$THINK, and $c=$EXPERIENCE. The two values will not sum up to one -- remember that we dropped a denominator somewhere on the way. \n",
    "\n",
    "**For this problem, please do the calculations by hand, and show your work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(THINK) * P(test|THINK) = \\frac{12}{27} * \\frac{6}{108} * \\frac{7}{108} * \\frac{1}{108} * \\frac{2}{108} * \\frac{1}{108} * \\frac{1}{108} * \\frac{1}{108} * \\frac{1}{108} * \\frac{1}{108} = 1.867e-17$\n",
    "\n",
    "<br>\n",
    "\n",
    "$P(EXPERIENCE)*P(test|EXPERIENCE) = \\frac{15}{27} * \\frac{9}{102}* \\frac{2}{102}* \\frac{1}{102}* \\frac{1}{102}* \\frac{5}{102}* \\frac{4}{102}* \\frac{1}{102}* \\frac{4}{102}* \\frac{1}{102} = 6.694e-16$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 Naive Bayes: Computing the probability of each target class with Python (10 pts.)\n",
    "\n",
    "For this problem, please answer the same question as in problem 2, but this time do it with Python.\n",
    "\n",
    "Here is the training data again, this time as Python data structures. Here is a list of the training sentences, with the :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feel_training_data = [\n",
    "    'I began to some guilt', \n",
    "    'I sorry for you Nardo', \n",
    "    'I the surge of adrenaline that comes right when your body realizes that its too late to save itself', \n",
    "    'It just like it was before', \n",
    "    'I still a little woozy', \n",
    "    'I pain under my jaw', \n",
    "    'I a tug on the back of my shirt', \n",
    "    'then I the pain withdraw and a lifeless voice said', \n",
    "    'I a warm touch on my cheek', \n",
    "    'I my leg turn', \n",
    "    'there is no need to troubled', \n",
    "    'he sorry for them', \n",
    "    'suddenly she a sharp pain in her jaw', \n",
    "    'Ms. Schaefer intense pain', \n",
    "    'yet we no longer uneasy', \n",
    "    'I like I was a big part of the team', \n",
    "    'they got help where they they needed it most', \n",
    "    'the team that they could part with the player', \n",
    "    'she that many pain specialists receive inadequate training', \n",
    "    'they that the police could not guarantee their safety', \n",
    "    'I have always that I fitted in there', \n",
    "    'your doctor that leg surgery is the best way to treat it', \n",
    "    \"many people have that they don't have any power\", \n",
    "    'I that I had so much to prove to my team', \n",
    "    'we like it was starting to be taken from us', \n",
    "    \"Jack he is hurting Boston's chances\", \n",
    "    'he this is the least risky path'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the gold labels for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feel_training_labels = ['EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'EXPERIENCE', 'EXPERIENCE', 'EXPERIENCE', \n",
    "                        'THINK', 'THINK', 'THINK', 'THINK', 'THINK', \n",
    "                        'THINK', 'THINK', 'THINK', 'THINK', 'THINK',\n",
    "                        'THINK', 'THINK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please again compute $P(THINK)$ and $P(EXPERIENCE)$, but **this time use Python code to do so**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(THINK) is 12 / 27.\n",
      "P(EXPERIENCE) is 15 / 27.\n"
     ]
    }
   ],
   "source": [
    "total = len(feel_training_labels)\n",
    "think_class = len([x for x in feel_training_labels if x == 'THINK'])\n",
    "exp_class = len([x for x in feel_training_labels if x == 'EXPERIENCE'])\n",
    "\n",
    "prob_think = think_class / total\n",
    "prob_experience = exp_class / total\n",
    "\n",
    "print(\"P(THINK) is {} / {}.\".format(think_class, total))\n",
    "print(\"P(EXPERIENCE) is {} / {}.\".format(exp_class, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Naive Bayes: Computing the probability of some features using Python (10 points)\n",
    "\n",
    "For this problem, please answer the same question as for problem 3, but this time please do this in Python. \n",
    "\n",
    "Here is the relevant list of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [\"I\", \"that\", \"there\", \"was\", \n",
    "                 \"a\", \"pain\", \"in\", \"my\",  \"leg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each feature in `test_features`, please compute its probability \n",
    "given THINK, and also its probability given EXPERIENCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for \"I\"\n",
      "\t given THINK: 6 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 9 / 102.\n",
      "\n",
      "Probability for \"that\"\n",
      "\t given THINK: 7 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 2 / 102.\n",
      "\n",
      "Probability for \"there\"\n",
      "\t given THINK: 1 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 1 / 102.\n",
      "\n",
      "Probability for \"was\"\n",
      "\t given THINK: 2 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 1 / 102.\n",
      "\n",
      "Probability for \"a\"\n",
      "\t given THINK: 1 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 5 / 102.\n",
      "\n",
      "Probability for \"pain\"\n",
      "\t given THINK: 1 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 4 / 102.\n",
      "\n",
      "Probability for \"in\"\n",
      "\t given THINK: 1 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 1 / 102.\n",
      "\n",
      "Probability for \"my\"\n",
      "\t given THINK: 1 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 4 / 102.\n",
      "\n",
      "Probability for \"leg\"\n",
      "\t given THINK: 1 / 108.\n",
      "\n",
      "\t given EXPERIENCE: 1 / 102.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_cl = list(zip(feel_training_data, feel_training_labels))\n",
    "sent_cl_dict = {'THINK': [], 'EXPERIENCE': []}\n",
    "exp_word_count = 0\n",
    "think_word_count = 0\n",
    "\n",
    "think_dict = {}\n",
    "exp_dict = {}\n",
    "\n",
    "for sent, cl in sent_cl:\n",
    "    if cl == 'EXPERIENCE':\n",
    "        sent = sent.split()\n",
    "        exp_word_count += len(sent)\n",
    "        sent_cl_dict[cl].append(sent)\n",
    "    else:\n",
    "        sent = sent.split()\n",
    "        think_word_count += len(sent)\n",
    "        sent_cl_dict[cl].append(sent)\n",
    "        \n",
    "for word in test_features: \n",
    "    print('Probability for \"{}\"'.format(word))\n",
    "    for key in sent_cl_dict:\n",
    "        occur = 0\n",
    "        for sentence in sent_cl_dict[key]:\n",
    "            occur += sentence.count(word)\n",
    "        if key == 'THINK':\n",
    "            print('\\t given THINK: {} / {}.'.format(occur, think_word_count))\n",
    "            think_dict[word] = occur / think_word_count\n",
    "            print()\n",
    "        else:\n",
    "            print('\\t given EXPERIENCE: {} / {}.'.format(occur, exp_word_count))\n",
    "            exp_dict[word] = occur / exp_word_count\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7: Naive Bayes: Computing the most likely class label for a datapoint with Python (10 points)\n",
    "\n",
    "For this problem, you need to again answer the same question as in problem 4, but this time find the solution using Python code: For the one datapoint in `wsd_test.txt`, which is the likeliest class, THINK or EXPERIENCE?\n",
    "\n",
    "For the test datapoint $d$, please compute both $P(THINK) P(d|THINK)$ and $P(EXPERIENCE) P(d|EXPERIENCE)$. Then say which one is bigger. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(THINK)P(d|THINK) = 1.867596143957447e-17\n",
      "P(EXPERIENCE)P(d|EXPERIENCE) = 6.694042126981267e-16\n",
      "Is the label EXPERIENCE more likely than THINK?: True\n"
     ]
    }
   ],
   "source": [
    "test_think = 1\n",
    "test_exp = 1\n",
    "    \n",
    "for prob in think_dict:\n",
    "    test_think *= think_dict[prob]\n",
    "    \n",
    "for prob in exp_dict:\n",
    "    test_exp *= exp_dict[prob]\n",
    "    \n",
    "test_think *= prob_think\n",
    "test_exp *= prob_experience\n",
    "\n",
    "print(\"P(THINK)P(d|THINK) = {}\".format(test_think))\n",
    "print(\"P(EXPERIENCE)P(d|EXPERIENCE) = {}\".format(test_exp))\n",
    "print(\"Is the label EXPERIENCE more likely than THINK?: \" + str(test_exp > test_think))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
