{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LIN 353C: Introduction to Computational Linguistics,  Fall 2022, Erk\n",
    "\n",
    "# Homework 6:  Part-of-speech tagging\n",
    "\n",
    "## Due: Wednesday November 2, end of day\n",
    "\n",
    "## Your name: Veronica Alejandro\n",
    "## Your EID: vaa678\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_6.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine. However, there is one command below (loading a gensim space) that may take a while, please plan for that. \n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Ambiguity resolved by part-of-speech tags. (24 points)\n",
    "\n",
    "Scour the web for 3 examples of newspaper headlines with odd alterna- tive interpretations, such as *British Left Waffles on Falkland Islands*, or *Iranian TV shows downed US drone*. Do not get them from sites that specialize in such headlines -- instead, get them from recent head-lines from actual news sites, like the BBC (Google news should be quite useful here as well). These cases are much more frequent than you would think; you will easily find enough examples.\n",
    "For each headline,\n",
    "\n",
    "* sketch the different readings that you perceive,\n",
    "* manually tag the headline, and\n",
    "* state whether knowledge of the part-of-speech tags removes the ambiguity.\n",
    "\n",
    "Use the following as your basic set of tags:\n",
    "\n",
    "* *V* verbs (sleeps, gave)\n",
    "* *DT* determiners (the, a)\n",
    "* *NN* common nouns (the dog)\n",
    "* *NNP* proper nouns (Europe, University of Texas at Austin) \n",
    "* *JJ* adjectives (smart, lucky)\n",
    "* *RB* adverbs (quietly, yesterday)\n",
    "* *P* preposition (of, on)\n",
    "* *CC* coordinating conjunction (and, or, but)\n",
    "\n",
    "You don’t need to worry about getting all your tags absolutely perfect. Just do what seems most sensible. In case you want to have more fine-grained tags, feel free to consult the Penn Treebank tagging guidelines: https://repository.upenn.edu/cis_reports/570/\n",
    "\n",
    "Tag each of the headlines using the word/tag format:\n",
    "\n",
    "    British/JJ Left/NN Waffles/V on/P Falkland/NNP Islands/NNP\n",
    "\n",
    "These kind of headlines are called crash blossoms. For the reason, see http://languagelog.ldc.upenn.edu/nll/?p=1693\n",
    "\n",
    "For an entertaining list of crash blossoms, see https://languagelog.ldc.upenn.edu/nll/?cat=118 (But do not use these for your homework.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **\"Buckingham Palace crowds feature in new exhibition\"**\n",
    "1. One meaning could be that the crows of people at Buckingham Palace are featured in a new exhibition. The second meaning could be that the Buckingham Palace crowds as in stands too close to a feature in a new exhibtion.\n",
    "2. First meaning: Buckingham/NNP Palace/NNP crowds/NN feature/V in/P new/JJ exhibition/NN.   <br>                              Second meaning: Buckingham/NNP Palace/NNP crowds/V feature/NN in/PP new/JJ exhibition/NN. \n",
    "3. Yes I do think that the part of speech tagging removes ambiguity. \n",
    "* **\"A new hard hat could help protect workers from on-the-job brain injuries\"**\n",
    "1. One meaning could be that there is a new hat that is hard. The second meaning is that there is a new hard hat. \n",
    "2. First meaning: A/DT new/JJ hard/NN hat/NN could/V help/V protect/V workers/N from/P on/P the/DT job/N brain/N injuries/N. <br> Second meaning: A/DT new/JJ hard/JJ hat/NN could/V help/V protect/V workers/N from/P on/P the/DT job/N brain/N injuries/N.\n",
    "3. Yes, I do think that the tags remove ambiguity.\n",
    "* **NYT column worried about liberal late-night comedy shows struggling**\n",
    "1. One meaning could be that there is are comedy shows, as in TV shows, are struggling. The second meaning is that late-night comedy is showing struggling. \n",
    "2. First meaning: NYT/NNP column/NN worried/V about/P liberal/JJ late-night/JJ comedy/N shows/N struggling/V\n",
    "<BR> Second meaning: NYT/NNP column/NN worried/V about/P liberal/JJ late-night/JJ comedy/N shows/V struggling/V\n",
    "3. Yes, the tags remove ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Thinking about part-of-speech tagging (30 pts.)\n",
    "\n",
    "## Part a\n",
    "\n",
    "Among the different types of part-of-speech taggers in chapter 5 of the NLTK book was the Default Tagger, which tags all words with the same part of speech. Name two reasons why it make sense to have such a tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It is important to have a baseline in order to properly evaluate tagger performance. <br>\n",
    "2. It is useful during the backoff method as the last method to tag words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b\n",
    "\n",
    "The Lookup Tagger in chapter 5 of the NLTK book first uses tagged data to determine the most frequent tag for each of the n most frequent words in the corpus. It then remembers this most frequent tag for each of these words, and always assigns that.\n",
    "We added a \"backoff\" such that for words that are not in its list of remembered words with tags, it always guesses “noun”.\n",
    "\n",
    "Chapter 5 of the NLTK book shows a graph (Figure 4.2) that charts the number of most frequent words used against the accuracy that the tagger achieves.\n",
    "If you use the 100 most frequent words in the news section of the Brown corpus, then the tagger already reaches an accuracy of 46% on that corpus. Why is it that this simple tagger gets such a relatively high value?\n",
    "If you keep adding more and more words with their most frequent tags to this tagger, the model improves up until about 93% accuracy. Why does it not reach 100%?\n",
    "And why do you think it is that in the beginning, adding more words makes the model improve fast, but after a while, the performance levels off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that in the beginning, adding more words makes the model improve faster because as you add more words, you also add new part of speech tags. After a while, the performance levels off because there are only so many part of speech tags that the tagger can learn. I think the tagger never reaches a 100% accuracy because words can have multiple part of speech tags, for example, the word *leave* can be either a noun or a verb, but only one can have a most likely tag. Thus, we could get instances where we use the noun but the tagger assigns the verb tag. Fianlyl, I think that this simple tagger get a high accuracy because it uses the most likely tag so there is a higher chance that the tag assigned is the correct one. Additionally, having the backoff method in place helps a lot for words that do not have a tag associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c\n",
    "\n",
    "If you evaluate this Lookup Tagger using all the words in the Brown news section, it achieves an accuracy of 93%. If you evaluate the same tagger (trained on Brown news) on the fiction part of the Brown corpus, you get 80% accuracy. Those are quite different numbers. Which of those two numbers should you trust more? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should trust the 80% accuracy because it is a different corpus than the one the tagger was trained on. We can't use the same corpus as the one the tagger was trained on since it gives us an inaccurate picture of the tagger's performance since it was already trained on the news dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Using a Hidden Markov Model to determine the probability of a tag sequence (40 pts.) \n",
    "\n",
    "\n",
    "The English Wikipedia has this to say about garden path sentences:\n",
    "\n",
    ">A garden path sentence, such as “The old man the boat” (meaning “Old people are the crew of the boat”), is a grammatically correct sentence that starts in such a way that a reader’s most likely interpretation will be incorrect; the reader is lured into a parse that turns out to be a dead end or yields a clearly unintended meaning. “Garden path” refers to the saying “to be led down [or up] the garden path”, meaning to be deceived, tricked, or seduced.\n",
    "\n",
    "Another well-known garden path sentence is\n",
    "\n",
    "> The complex houses married and single soldiers and their families\n",
    "\n",
    "For this problem, you will do part-of-speech tagging for a simplified version of this sentence: \n",
    "\n",
    "> the complex houses students\n",
    "\n",
    "Here are the transition probabilities:\n",
    "\n",
    "* **START**: P(DT|START)=0.14 \n",
    "* **DT**: P(JJ|DT) = 0.23, P(NN|DT) = 0.61\n",
    "* **JJ**: P(NN|JJ) = 0.64, P(VB|JJ) = 0.01\n",
    "* **NN**: P(NN|NN) = 0.1,  P(V B|NN) = 0.06, P(END|NN) = 0.003 \n",
    "* **VB**: P(NN|VB)=0.11\n",
    "\n",
    "Here are emission probabilities. Assume all other emission probabilities are zero:\n",
    "\n",
    "* **the:** P(the|DT)=0.63\n",
    "* **complex:** P(complex|JJ) = 0.0008, P(complex|NN) = 0.0001\n",
    "* **houses:** P(houses|NN) = 0.0003, P(houses|V B) = 0.00003 \n",
    "* **students:** P(students|NN) = 0.0009\n",
    "\n",
    "What is the probability of the following tag sequence? \n",
    "\n",
    "> START DT NN VB NN\n",
    "\n",
    "Please do this \"by hand\", and show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * P(the|DT) * P(DT|START) * P(complex|NN) * P(NN|DT) * P(houses|VB) * P(VB|NN) * P(students|NN) * P(NN|VB)\n",
    " * = 0.63 * 0.14 * 0.0001 * 0.61 * 0.00003 * 0.06 * 0.0009 * 0.11\n",
    " * = 9.5875164e-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Viterbi (6 pts.)\n",
    "\n",
    "The Viterbi algorithm is used to determine the best tag sequence for a given word sequence. It is a dynamic programming approach, meaning that it stores intermediate results instead of recomputing them, in order to bring the runtime down a lot. \n",
    "\n",
    "The intermediate results that are being stored in the Viterbi algorithm are probabilities $v_t(j)$, meaning the probability of the most likely path ending in state (POS tag) $j$ in step $t$ (at the $t$-th word). \n",
    "\n",
    "The Viterbi probability $v_t(j)$ of the most likely path ending in $j$ and emitting word $o_t$ is\n",
    "\n",
    "$v_t(j) = max_i v_{t-1}(i) a_{ij} b_j(o_t)$\n",
    "\n",
    "So for each tag $i$, we look at the probability of the most likely path that ends at $i$ at step $t-1$, multiply it with the probability of transitioning from $i$ to $j$ (that is $a_{ij}$) and with the probability of emitting the $t$-th observation from state (POS tag) $j$ (this is $b_j$). We compare all these probabilities, and take the maximum. This, then, is the probability of the likeliest path ending at $j$ at step $t$. \n",
    "\n",
    "We back to the sentence \"the complex houses students\" from the previous sentence. \n",
    "\n",
    "Say we have this for observation 2, \"complex\":\n",
    "* $v_2(JJ) = 2e-4$\n",
    "* $v_2(NN) = 5.5e-6$\n",
    "* and $v_2$ is zero for all other tags. \n",
    "\n",
    "Then what is $v_3(VB)$, the probability of the likeliest path that ends at tag VB in step 3, that is, that tags \"houses\" with VB? So here $t=3$, $j = VB$, and you have to compare tags $i$ that are either JJ or NN. \n",
    "\n",
    "Do this by hand, and show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **i = JJ, j = VB\n",
    "1. $a_{ij}$ = 0.01\n",
    "2. $b_j{o_t}$ = 0.00003\n",
    "3. $v_2(i)$ = 2e-4\n",
    "4. $v_3(VB)$ = 6e-11\n",
    "* **i = NN, j = VB\n",
    "1. $a_{ij}$ = 0.006\n",
    "2. $b_j{o_t}$ = 0.00003\n",
    "3. $v_2(i)$ = 5.5e-6\n",
    "4. $v_3(VB)$ = 9.9e-13"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
