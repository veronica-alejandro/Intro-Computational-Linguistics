{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 353C: Introduction to Computational Linguistics,  Fall 2020, Erk\n",
    "\n",
    "# Homework 7:  Chunking, evaluation, and context-free grammars\n",
    "\n",
    "## Due: Thursday November 11, end of day\n",
    " \n",
    "## Your name: Veronica Alejandro\n",
    "## Your EID: vaa678\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_7.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine. However, there is one command below (loading a gensim space) that may take a while, please plan for that. \n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Chunking (30 points)\n",
    "\n",
    "For this problem, you will create a noun phrase chunker and evaluate it. \n",
    "\n",
    "You can use any of the chunking methods described in the NLTK book at\n",
    "https://www.nltk.org/book_1ed/ch07.html\n",
    "\n",
    "Call\n",
    "\n",
    "`nltk.app.chunkparser()`\n",
    "\n",
    "to enter the interactive chunker development and analysis platform that NLTK offers.\n",
    "\n",
    "**Important: Please check soon that the NLTK chunk parser app works on your machine.**\n",
    "\n",
    "Note: This app uses the *conll2000* data. If you haven't downloaded the data, you may get an error that mentions\n",
    "\"OSError: No such file or directory:...conll2000/train.txt\". In that case, please run the command\n",
    "\n",
    "```nltk.download('conll2000')```\n",
    "\n",
    "If you still cannot get the chunk parser app to work, check the notebook `Chunking without the app.ipynb`, which shows you how to do the homework without the app.\n",
    "\n",
    "For your chunker, use at least 5 rules that are not used in the NLTK book and that are not the rule\n",
    "`{ <DT><JJS?>*<NNP?> }`\n",
    "that we used in class. You are also allowed to use a single rule that extends the NLTK book rules, or extends the rule above from in class, in 5 ways. \n",
    "\n",
    "Note that you will need to put curly brackets around your rules.\n",
    "Copy your chunker rules into the box below. Also report the precision and recall that your chunker achieves. (They are listed at the bottom of the chunker app window.)\n",
    "\n",
    "In addition, describe, in the box below, at least two examples that your chunker mis-analyzes. Discuss what the problem is and what you might do to address it. You can view additional sentences by clicking the “Next example” button, or using Control-n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('conll2000')\n",
    "nltk.app.chunkparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules: ####\n",
    "1. `{ <DT><NN>+ }`\n",
    "- Precision: 84.31%\n",
    "- Recall: 13.92%\n",
    "2. `{ <JJ><NNP>+ }`\n",
    "- Precision: 80.0%\n",
    "- Recall: 1.29%\n",
    "3. `{<DT><NNP>+<NN>+}`\n",
    "- Precision: 85.71%\n",
    "- Recall: 0.97%\n",
    "4. `{ <DT><JJS?>*<NNP?>*<NN>}`\n",
    "- Precision: 87.67%\n",
    "- Recall: 20.71%\n",
    "- This chunker mis-analyzed an example that started with a determiner and a noun, but the example also had CD tags and NNS tags that the chunker did not account for. We could build this into the chunker to account for them if they occur.\n",
    "5. `{ <DT><JJS?>*<NNP?>*<NN>*<CD?>*<NNS?>*}`\n",
    "- Precision: 80.49%\n",
    "- Recall: 26.70%\n",
    "- There was an example that started with a JJ tag that the chunker mis-analyzed because it only counted the latter half of the example after it started with the determiner tag. To improve on this, I think we could account for examples that don't only start with determiners, but also JJ, NP, PRP, etc. However, I'm thinking it would be a really longer regex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Evaluation with Precision and Recall (30 points)\n",
    "\n",
    "The chunker that you used in the previous problem was evaluated\n",
    "using Precision and Recall. They are computed as follows.\n",
    "\n",
    "* True Positives (TP) are word sequences that are actually noun\n",
    "phrases, and where the chunker says they are noun phrases.\n",
    "* False Positives (FP) are word sequences that the chunker says are noun phrases but that are not.\n",
    "* False Negatives (FN) are actual noun phrases that the chunker misses.\n",
    "* TrueNegatives(TN)are word sequences that are not noun phrases and that the chunker does not consider noun phrases.\n",
    "\n",
    "Then Precision is the fraction of actual noun phrases among the sequences that the chunker labels as noun phrases:\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "Recall is the fraction of actual noun phrases that the parses recognizes as noun phrases:\n",
    "\n",
    "$Recall = \\frac{TP}{TP+FN}$\n",
    "\n",
    "F-score combines Precision and Recall:\n",
    "\n",
    "$Fscore = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}$\n",
    "\n",
    "The (hypothetical) XYZ-Chunker achieves the following results on the (hypothetical) ABC-corpus:\n",
    "\n",
    "* True Positives: 511\n",
    "* False Positives: 83\n",
    "* False Negatives: 302 \n",
    "* True Negatives: 2508\n",
    "\n",
    "What are Precision, Recall, and F-score of the XYZ-Chunker? Show your work. (You don’t need to use Python to solve this problem, but you can if you like. Still, show your work.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8602693602693603\n",
      "Recall: 0.6285362853628537\n",
      "F-score: 0.72636815920398\n"
     ]
    }
   ],
   "source": [
    "precision = 511 / (511 + 83)\n",
    "recall = 511 / (511 + 302)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "print('Precision: ' + str(precision))\n",
    "print('Recall: ' + str(recall))\n",
    "print('F-score: ' + str(fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Context-free grammar (40 points)\n",
    "\n",
    "English has an impoverished system for marking case that only shows up on pronouns. For example, *I*, *he*, and *they* have nominative case, while *me*, *him*, and *them* have accusative case. \n",
    "\n",
    "Write a context-free grammar that accepts the sentences in (a) but not those in (b):\n",
    "\n",
    "## (a) Sentences to accept:\n",
    "\n",
    "* i. she sees him\n",
    "* ii. they see him\n",
    "* iii. they see the woman\n",
    "* iv. they see the women\n",
    "* v. I know her\n",
    "* vi. I know the man whom she sees\n",
    "* vii. I know the woman who sees him\n",
    "* viii. the woman who sees him walks (that is, the woman is doing the walking)\n",
    "* ix. the women who see him walk (that is, the women are doing the walking)\n",
    "    \n",
    "## (b) Sentences to not accept: \n",
    "\n",
    "(I am putting a star in front of each sentence to indicate that they are not grammatical.)\n",
    "\n",
    "* i. *she sees he\n",
    "* ii. *they sees him\n",
    "* iii. *me know her\n",
    "* iv. *I know the woman whom sees him\n",
    "* v. *the woman who sees him walk \n",
    "* vi. *the woman who see him walks \n",
    "* vii. *the women who sees him walk viii. \n",
    "* *the women who see him walks\n",
    "\n",
    "Please put your context-free grammar below.\n",
    "\n",
    "Additionally, draw the tree structure for the sentence \n",
    "    I know the man whom she sees.\n",
    "You can either do that here using ASCII art, or do it on paper and submit a clearly readable photo of the tree structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* she --> NP(nom, 3, s)\n",
    "* they --> NP(nom, 3, p)\n",
    "* I --> NP(nom, not3, s)\n",
    "* he --> NP(nom, 3, s)\n",
    "* her --> NP(acc, 3, s)\n",
    "* him --> NP(acc, 3, s)\n",
    "* me --> NP(acc, 1, s)\n",
    "* man --> N(s)\n",
    "* woman --> N(s)\n",
    "* women --> N(p)\n",
    "* the --> Det\n",
    "* who --> comp_1\n",
    "* whom --> comp_2\n",
    "* see --> V(not 3)\n",
    "* sees --> V(3)\n",
    "* know --> V(not 3)\n",
    "* walk --> V(not 3)\n",
    "* walks --> V(3)\n",
    "### Key ###\n",
    "- nom = nominative\n",
    "- acc = accusative\n",
    "- 3 = 3rd person\n",
    "- 1 = 1st person\n",
    "- s = singular\n",
    "- p = plural\n",
    "- comp = complementizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S --> NP(nom, 3, s) VP(3, s)\n",
    "- S --> NP(nom, not(3, s)) VP(not(3,s))\n",
    "\n",
    "* NP(nom, 3, s) --> (Det) N(S)\n",
    "* NP(nom, not(3, s)) --> (Det) N(P)\n",
    "\n",
    "* NP(acc) --> (Det) N (COMP)\n",
    "* VP(3, s) --> V(3) (NP(ACC))\n",
    "* VP(not(3, s)) --> V(not3) (NP(ACC))\n",
    "* COMPP --> COMP1 VP(3S)\n",
    "* COMPP --> COMP1 VP(not(3, s))\n",
    "* COMPP --> COMP2 NP(nom, 3, s) VP(3, s)\n",
    "* COMPP --> COMP2 NP(nom, not(3, s)) VP(not(3, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted tree as a separate file. Worked with Ethan Glass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
