{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LIN 353C Final Project: Extractive Text Summarization #\n",
        "\n",
        "Our goal for this project is to perform extractive text summarization through various methods, from the most basic to more complicated methods. For this project, we will use the CNN/Daily Mail dataset."
      ],
      "metadata": {
        "id": "qPnoKYPVODqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 01. Importing libraries + downloading packages"
      ],
      "metadata": {
        "id": "V8oaOAroVcAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "56bHt2OoizFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce72852e-d9ac-4f4d-ab85-328d072378b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "###\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpz36_qdx3I2",
        "outputId": "16358280-cfcc-41c8-a851-0981b65b9734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from rouge import FilesRouge\n",
        "import gensim.downloader as gensim_api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx\n",
        "\n",
        "import math\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "-NPV_PRJVbY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 02. Downloading + processing data ###"
      ],
      "metadata": {
        "id": "5DdoPYj-O0rS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8h7EcopOAr7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "1bd99d18-1b0f-4475-9412-094bd0efe6fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             article  \\\n",
              "0  Ever noticed how plane seats appear to be gett...   \n",
              "1  A drunk teenage boy had to be rescued by secur...   \n",
              "2  Dougie Freedman is on the verge of agreeing a ...   \n",
              "3  Liverpool target Neto is also wanted by PSG an...   \n",
              "4  Bruce Jenner will break his silence in a two-h...   \n",
              "\n",
              "                                          highlights  \n",
              "0  Experts question if  packed out planes are put...  \n",
              "1  Drunk teenage boy climbed into lion enclosure ...  \n",
              "2  Nottingham Forest are close to extending Dougi...  \n",
              "3  Fiorentina goalkeeper Neto has been linked wit...  \n",
              "4  Tell-all interview with the reality TV star, 6...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-53d54535-409c-49ad-8729-69dd98bb18f6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ever noticed how plane seats appear to be gett...</td>\n",
              "      <td>Experts question if  packed out planes are put...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A drunk teenage boy had to be rescued by secur...</td>\n",
              "      <td>Drunk teenage boy climbed into lion enclosure ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dougie Freedman is on the verge of agreeing a ...</td>\n",
              "      <td>Nottingham Forest are close to extending Dougi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Liverpool target Neto is also wanted by PSG an...</td>\n",
              "      <td>Fiorentina goalkeeper Neto has been linked wit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bruce Jenner will break his silence in a two-h...</td>\n",
              "      <td>Tell-all interview with the reality TV star, 6...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53d54535-409c-49ad-8729-69dd98bb18f6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-53d54535-409c-49ad-8729-69dd98bb18f6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-53d54535-409c-49ad-8729-69dd98bb18f6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# turn data into a dataframe and save the first 50 articles\n",
        "test_data = pd.read_csv('/content/drive/My Drive/test.csv', encoding = 'utf-8')\n",
        "df = test_data.iloc[:50]\n",
        "\n",
        "# we don't need the id column\n",
        "del df['id']\n",
        "\n",
        "for i in range(50):\n",
        "  # replace (CNN) tag from articles\n",
        "  if '(CNN)' in df.at[i, 'article']:\n",
        "    df.at[i, 'article'] = df.at[i, 'article'].replace('(CNN)', '')\n",
        "\n",
        "# create a list containing each highlight\n",
        "highlights_lst = []\n",
        "for highlight in df['highlights']:\n",
        "  highlight = highlight.replace('\\n', '')\n",
        "  highlights_lst.append(highlight)\n",
        "\n",
        "# create a file for the highlights dataframe -- needed for rouge evaluation\n",
        "hl_df = pd.DataFrame({'highlights': highlights_lst})\n",
        "hl_df.to_csv('highlights.csv')\n",
        "\n",
        "df.head()\n",
        "\n",
        "# turing articles_lst into a dictionary of form {article_name: {word: count,...}}--- i'm not sure about this either\n",
        "# article_freq_dict = {}\n",
        "# inter_dict = {}\n",
        "# for i in range(len(articles_lst)):\n",
        "#     name = 'article' + str(i)\n",
        "#     for sent in articles_lst[i]:\n",
        "#         article_freq_dict[name] = Counter(sent)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text processing and cleaning\n",
        "tokenized_articles = []\n",
        "tokenized_sentences = []\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for article in df['article']:\n",
        "  paragraph = []\n",
        "  # sentence tokenize the article\n",
        "  sentences = nltk.tokenize.sent_tokenize(article)\n",
        "  tokenized_sentences.append(sentences)\n",
        "\n",
        "  for sentence in sentences:\n",
        "    # lowercase everything\n",
        "    sentence = sentence.lower()\n",
        "    # remove everything that is not a letter\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "    # word tokenize each sentence\n",
        "    words = nltk.tokenize.word_tokenize(sentence)\n",
        "\n",
        "    # remove stopwords\n",
        "    clean_sentence = []\n",
        "    for word in words:\n",
        "      if word not in stop_words:\n",
        "        clean_sentence.append(word)\n",
        "\n",
        "    # paragraph is a list of lists containing each sentence in the article\n",
        "    paragraph.append(clean_sentence)\n",
        "\n",
        "\n",
        "  tokenized_articles.append(paragraph)\n",
        "\n",
        "# example article below\n",
        "print(tokenized_articles[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBysd3GuPK4k",
        "outputId": "5ec1d479-911b-4211-8474-48694c826499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['ever', 'noticed', 'plane', 'seats', 'appear', 'getting', 'smaller', 'smaller'], ['increasing', 'numbers', 'people', 'taking', 'skies', 'experts', 'questioning', 'packed', 'planes', 'putting', 'passengers', 'risk'], ['say', 'shrinking', 'space', 'aeroplanes', 'uncomfortable', 'putting', 'health', 'safety', 'danger'], ['squabbling', 'arm', 'rest', 'shrinking', 'space', 'planes', 'putting', 'health', 'safety', 'danger'], ['week', 'u', 'consumer', 'advisory', 'group', 'set', 'department', 'transportation', 'said', 'public', 'hearing', 'government', 'happy', 'set', 'standards', 'animals', 'flying', 'planes', 'stipulate', 'minimum', 'amount', 'space', 'humans'], ['world', 'animals', 'rights', 'space', 'food', 'humans', 'said', 'charlie', 'leocha', 'consumer', 'representative', 'committee'], ['time', 'dot', 'faa', 'take', 'stand', 'humane', 'treatment', 'passengers'], ['could', 'crowding', 'planes', 'lead', 'serious', 'issues', 'fighting', 'space', 'overhead', 'lockers', 'crashing', 'elbows', 'seat', 'back', 'kicking'], ['tests', 'conducted', 'faa', 'use', 'planes', 'inch', 'pitch', 'standard', 'airlines', 'decreased'], ['many', 'economy', 'seats', 'united', 'airlines', 'inches', 'room', 'airlines', 'offer', 'little', 'inches'], ['cynthia', 'corbertt', 'human', 'factors', 'researcher', 'federal', 'aviation', 'administration', 'conducts', 'tests', 'quickly', 'passengers', 'leave', 'plane'], ['tests', 'conducted', 'using', 'planes', 'inches', 'row', 'seats', 'standard', 'airlines', 'decreased', 'reported', 'detroit', 'news'], ['distance', 'two', 'seats', 'one', 'point', 'seat', 'point', 'seat', 'behind', 'known', 'pitch'], ['airlines', 'stick', 'pitch', 'inches', 'fall'], ['united', 'airlines', 'inches', 'space', 'gulf', 'air', 'economy', 'seats', 'inches', 'air', 'asia', 'offers', 'inches', 'spirit', 'airlines', 'offers', 'inches'], ['british', 'airways', 'seat', 'pitch', 'inches', 'easyjet', 'inches', 'thomson', 'short', 'haul', 'seat', 'pitch', 'inches', 'virgin', 'atlantic']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokenized_articles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYedELCDHpYi",
        "outputId": "d96aafee-6073-41f8-9b89-d1bbdfdbdb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 03. Text summarization###"
      ],
      "metadata": {
        "id": "pJaPGnAVn0S2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First-sentence method"
      ],
      "metadata": {
        "id": "i16JdJPbAlDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in this section, fs means first-sentence\n",
        "\n",
        "# create a list for each summary\n",
        "fs_lst = []\n",
        "\n",
        "for article in df['article']:\n",
        "  # tokenize to get the first sentence\n",
        "  new = nltk.tokenize.sent_tokenize(article)\n",
        "  summary = new[0]\n",
        "  fs_lst.append(summary)\n",
        "\n",
        "# create a dataframe for summary\n",
        "# and create a file since we will need a file to evaluate using rouge\n",
        "fs_df = pd.DataFrame({'summary': fs_lst})\n",
        "fs_df.to_csv('first_sentence.csv')"
      ],
      "metadata": {
        "id": "wEqRxL3FpgzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TextRank using word embeddings"
      ],
      "metadata": {
        "id": "NDLdyrQAAael"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we use the GloVe embeddings trained on Wikipedia 2014 + Gigaword 5\n",
        "# extract 100 dimension vectors\n",
        "space = gensim_api.load(\"glove-wiki-gigaword-100\")"
      ],
      "metadata": {
        "id": "_t9VBJPWAlf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a997a9ac-b33e-40a7-a562-20d1e75cd502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# now we get the vector representation of each sentence using word embeddings\n",
        "# we will get the word embeddings for each word in each sentence, then take the avg of each sentence\n",
        "from numpy.linalg import norm\n",
        "article_scores = []\n",
        "\n",
        "for article in tokenized_articles:\n",
        "  sentence_vectors = []\n",
        "\n",
        "  for sentence in article:\n",
        "    # we create a zero vector of dim 100\n",
        "    sentence_vector = np.zeros((100,))\n",
        "\n",
        "    for word in sentence:\n",
        "      # get the word embedding from gensim, skip if it doesn't exist\n",
        "      try:\n",
        "        vector = space[word]\n",
        "      except:\n",
        "        continue\n",
        "      # add all vectors from each sentence\n",
        "      sentence_vector += vector\n",
        "      # divide by sentence size to get avg\n",
        "      sentence_vector /= len(sentence)\n",
        "\n",
        "    # append each sentence's avg vector to a list\n",
        "    sentence_vectors.append(sentence_vector)\n",
        "\n",
        "  dim = len(article)\n",
        "  matrix = np.zeros([dim, dim])\n",
        "\n",
        "  # now calculate the cosine similarity between each sentence vector\n",
        "  # to form a similarity matrix\n",
        "  for i in range(dim):\n",
        "    for j in range(dim):\n",
        "      if i != j:\n",
        "        matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1, 100), sentence_vectors[j].reshape(1, 100))\n",
        "\n",
        "  # turn similarity matrix into a pagerank graph using networkx\n",
        "  graph = nx.from_numpy_array(matrix)\n",
        "  scores = nx.pagerank_numpy(graph)\n",
        "\n",
        "  # get the sentence number for top 5 sentences with the highest scores\n",
        "  indexes = list(dict(sorted(scores.items(), key = lambda x: x[1])))[:3]\n",
        "  article_scores.append(indexes)\n",
        "\n",
        "print(article_scores[0])\n",
        "assert len(article_scores) == 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWgjTJhkbvu5",
        "outputId": "1ce2ecf7-7331-4dc3-c6dc-5a57a9cc7327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 7, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get summaries of top 5 sentences using the scores we got for each sentence\n",
        "embeddings_summaries = []\n",
        "\n",
        "for n in range(50):\n",
        "  summary = ''\n",
        "  for idx in article_scores[n]:\n",
        "    summary += tokenized_sentences[n][idx]\n",
        "    summary += ' '\n",
        "  embeddings_summaries.append(summary)\n",
        "\n",
        "# create a dataframe for summary\n",
        "# and create a file since we will need a file to evaluate using rouge\n",
        "# we = word embeddings\n",
        "\n",
        "we_df = pd.DataFrame({'summary': embeddings_summaries})\n",
        "we_df.to_csv('word_embeddings.csv')\n",
        "print(we_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSceqfgOv4FD",
        "outputId": "58c58c26-5fa1-46a2-af49-2645dc18af6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             summary\n",
            "0  'In a world where animals have more rights to ...\n",
            "1  A police spokesman said: 'He has been cautione...\n",
            "2  That has not prevented Forest's ownership maki...\n",
            "3  Liverpool target Neto is also wanted by PSG an...\n",
            "4  'Bruce had silicone breast implants put in a f...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TF-IDF(Article)"
      ],
      "metadata": {
        "id": "zQ7isyfBAvpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# going from tokenized_articles, which is list of articles which is list of setnence which is list of word tokens,\n",
        "# to frequency dictionary for each article:\n",
        "article_freq_dict = {}\n",
        "\n",
        "for i in range(len(tokenized_articles)):\n",
        "    obj = Counter()\n",
        "    name = 'article' + str(i)\n",
        "    flat_list = []\n",
        "    for sentence in tokenized_articles[i]:\n",
        "        flat_list += sentence\n",
        "        dict.update(obj, Counter(flat_list))\n",
        "    article_freq_dict[name] = obj\n",
        "\n",
        "# article_freq_dict looks like: {'article0': Counter{'the': 20, .......}}"
      ],
      "metadata": {
        "id": "d0WVjTIu_ior"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my tf-idf function:\n",
        "\n",
        "def get_tf_idf(article, word, freq_dict):\n",
        "    tf = math.log(freq_dict[article][word]+1)\n",
        "    count = 0\n",
        "    for i in range(len(freq_dict)):\n",
        "        if word in  article_freq_dict['article'+str(i)]:\n",
        "            count+= 1\n",
        "    idf = math.log(len(freq_dict)/count)\n",
        "    return tf*idf\n",
        "\n",
        "# this returns a value"
      ],
      "metadata": {
        "id": "JVO2USuWJJHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# my scoring function\n",
        "\n",
        "def tf_idf_scorer(article, full_text, freq_dict):\n",
        "    scores = {}\n",
        "    article_text = full_text[int(article[-1])]\n",
        "    for i in range(len(article_text)):\n",
        "        scores[\"sentence\"+str(i)] = 0\n",
        "        for k in range(len(article_text[i])):\n",
        "            scores[\"sentence\"+str(i)] += get_tf_idf(article, article_text[i][k], freq_dict)\n",
        "    return scores\n",
        "\n",
        "# this returns a dictionary of form: {'sentence0': tf-df value, ....}"
      ],
      "metadata": {
        "id": "6marXJ3CJOd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this returns the sentences with the highest scores\n",
        "\n",
        "def summarizer(article, fully_tokenized, sentences, freq_dict, number):\n",
        "    scores = tf_idf_scorer(article, fully_tokenized, freq_dict)\n",
        "    ordered_scores = sorted(scores.items(), key = lambda x:x[1], reverse = True)\n",
        "    article_text = fully_tokenized[int(article[-1])]\n",
        "    inter_summary = []\n",
        "\n",
        "    inter_summary += (ordered_scores[:number])\n",
        "\n",
        "    indexes = []\n",
        "    for tup in inter_summary:\n",
        "        if tup[0][-2:].isdigit():\n",
        "            indexes.append(int(tup[0][-2:]))\n",
        "        else:\n",
        "            indexes.append(int(tup[0][-1:]))\n",
        "\n",
        "    summary_list = []\n",
        "    for index in indexes:\n",
        "        summary_list.append(sentences[int(article[-1])][index])\n",
        "\n",
        "    summary = ''\n",
        "    for sentence in summary_list:\n",
        "        for word in sentence:\n",
        "            summary += word\n",
        "\n",
        "\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "bCK4mEWDJW1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_sentences[0])\n",
        "print(tokenized_articles[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMOZSZDLwmKo",
        "outputId": "f64edf9c-9222-475d-cdcc-088c8a840c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ever noticed how plane seats appear to be getting smaller and smaller?', 'With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk.', \"They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger.\", 'More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger?', \"This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.\", \"'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.\", \"'It is time that the DOT and FAA take a stand for humane treatment of passengers.'\", 'But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking?', 'Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased .', 'Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches .', 'Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane.', 'But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.', 'The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch.', 'While most airlines stick to a pitch of 31 inches or above, some fall below this.', 'While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches.', \"British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\"]\n",
            "[['ever', 'noticed', 'plane', 'seats', 'appear', 'getting', 'smaller', 'smaller'], ['increasing', 'numbers', 'people', 'taking', 'skies', 'experts', 'questioning', 'packed', 'planes', 'putting', 'passengers', 'risk'], ['say', 'shrinking', 'space', 'aeroplanes', 'uncomfortable', 'putting', 'health', 'safety', 'danger'], ['squabbling', 'arm', 'rest', 'shrinking', 'space', 'planes', 'putting', 'health', 'safety', 'danger'], ['week', 'u', 'consumer', 'advisory', 'group', 'set', 'department', 'transportation', 'said', 'public', 'hearing', 'government', 'happy', 'set', 'standards', 'animals', 'flying', 'planes', 'stipulate', 'minimum', 'amount', 'space', 'humans'], ['world', 'animals', 'rights', 'space', 'food', 'humans', 'said', 'charlie', 'leocha', 'consumer', 'representative', 'committee'], ['time', 'dot', 'faa', 'take', 'stand', 'humane', 'treatment', 'passengers'], ['could', 'crowding', 'planes', 'lead', 'serious', 'issues', 'fighting', 'space', 'overhead', 'lockers', 'crashing', 'elbows', 'seat', 'back', 'kicking'], ['tests', 'conducted', 'faa', 'use', 'planes', 'inch', 'pitch', 'standard', 'airlines', 'decreased'], ['many', 'economy', 'seats', 'united', 'airlines', 'inches', 'room', 'airlines', 'offer', 'little', 'inches'], ['cynthia', 'corbertt', 'human', 'factors', 'researcher', 'federal', 'aviation', 'administration', 'conducts', 'tests', 'quickly', 'passengers', 'leave', 'plane'], ['tests', 'conducted', 'using', 'planes', 'inches', 'row', 'seats', 'standard', 'airlines', 'decreased', 'reported', 'detroit', 'news'], ['distance', 'two', 'seats', 'one', 'point', 'seat', 'point', 'seat', 'behind', 'known', 'pitch'], ['airlines', 'stick', 'pitch', 'inches', 'fall'], ['united', 'airlines', 'inches', 'space', 'gulf', 'air', 'economy', 'seats', 'inches', 'air', 'asia', 'offers', 'inches', 'spirit', 'airlines', 'offers', 'inches'], ['british', 'airways', 'seat', 'pitch', 'inches', 'easyjet', 'inches', 'thomson', 'short', 'haul', 'seat', 'pitch', 'inches', 'virgin', 'atlantic']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_summaries = []\n",
        "for i in range(50):\n",
        "  tf_idf_summaries.append((summarizer('article'+str(i), tokenized_articles, tokenized_sentences, article_freq_dict, 5)))\n",
        "\n",
        "tf_idf_df = pd.DataFrame({'summary': tf_idf_summaries})\n",
        "tf_idf_df.to_csv('tf_idf.csv')\n",
        "print(tf_idf_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flecfYPIKEX7",
        "outputId": "4514fe13-3f97-4bb2-f679-7c945a37f062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             summary\n",
            "0  While United Airlines has 30 inches of space, ...\n",
            "1  Next level drunk: Intoxicated Rahul Kumar, 17,...\n",
            "2  Freedman has stabilised Forest since he replac...\n",
            "3  Liverpool target Neto is also wanted by PSG an...\n",
            "4  Speaking out: Bruce Jenner, pictured on 'Keepi...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(summarizer('article0', tokenized_articles, tokenized_sentences, article_freq_dict, 5))"
      ],
      "metadata": {
        "id": "_6qmkgY9JZC-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4eb0c40-defa-4539-fac8-71157dd238eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches.British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News.Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches .This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ALT TF-IDF (SENTENCES)"
      ],
      "metadata": {
        "id": "xV39xxAJQQ5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alt_freq_dict = {}\n",
        "inter_dict = {}\n",
        "\n",
        "for i in range(len(tokenized_articles)):\n",
        "    name = 'article' + str(i)\n",
        "    inter_dict = {}\n",
        "    for k in range(len(tokenized_articles[i])):\n",
        "        inter_dict['sentence'+str(k)] = Counter(tokenized_articles[i][k])\n",
        "\n",
        "    alt_freq_dict[name] = inter_dict"
      ],
      "metadata": {
        "id": "oECSimvtoiwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alt_get_tf_idf(article, sentence, word, fully_tokenized, freq_dict):\n",
        "    tf = math.log(freq_dict[article][sentence][word]+1)\n",
        "    count = 0\n",
        "    for sent in fully_tokenized[int(article[-1])]:\n",
        "        if word in sent:\n",
        "            count+= 1\n",
        "    idf = math.log(len(fully_tokenized[int(article[-1])])/count)\n",
        "    return tf*idf"
      ],
      "metadata": {
        "id": "fW--hQYDQVsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alt_tf_idf_scorer(article, fully_tokenized, freq_dict):\n",
        "    scores = {}\n",
        "    article_text = fully_tokenized[int(article[-1])]\n",
        "    for i in range(len(article_text)):\n",
        "        sentence = 'sentence'+str(i)\n",
        "        scores[sentence] = 0\n",
        "        for k in range(len(article_text[i])):\n",
        "            scores[sentence] += alt_get_tf_idf(article, sentence, article_text[i][k], fully_tokenized, freq_dict)\n",
        "    return scores"
      ],
      "metadata": {
        "id": "_IpPYVbIQsqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alt_summarizer(article, fully_tokenized, sentences, freq_dict, number):\n",
        "    scores = alt_tf_idf_scorer(article, fully_tokenized, freq_dict)\n",
        "    ordered_scores = sorted(scores.items(), key = lambda x:x[1], reverse = True)\n",
        "    article_text = fully_tokenized[int(article[-1])]\n",
        "    inter_summary = []\n",
        "\n",
        "    inter_summary += (ordered_scores[:number])\n",
        "\n",
        "    indexes = []\n",
        "    for tup in inter_summary:\n",
        "        if tup[0][-2:].isdigit():\n",
        "            indexes.append(int(tup[0][-2:]))\n",
        "        else:\n",
        "            indexes.append(int(tup[0][-1:]))\n",
        "\n",
        "    summary_list = []\n",
        "    for index in indexes:\n",
        "        summary_list.append(sentences[int(article[-1])][index])\n",
        "\n",
        "    summary = ''\n",
        "    for sentence in summary_list:\n",
        "        for word in sentence:\n",
        "            summary += word\n",
        "\n",
        "\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "Zc2GJnIlQulv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(alt_summarizer('article0', tokenized_articles, tokenized_sentences, alt_freq_dict, 3 ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzJMrDkLQxM8",
        "outputId": "a2a81eaa-0da9-47ce-fed7-299f6988046f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans.While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches.British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alt_tf_idf_summaries = []\n",
        "for i in range(50):\n",
        "  alt_tf_idf_summaries.append((alt_summarizer('article'+str(i), tokenized_articles, tokenized_sentences, alt_freq_dict, 5)))\n",
        "\n",
        "alt_tf_idf_df = pd.DataFrame({'summary': alt_tf_idf_summaries})\n",
        "alt_tf_idf_df.to_csv('alt_tf_idf.csv')\n",
        "print(alt_tf_idf_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "HFCjIANtQ96H",
        "outputId": "66727821-f57b-4955-e0b1-7a1b17eee304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-f5c88fcedabd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malt_tf_idf_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0malt_tf_idf_summaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malt_summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_articles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt_freq_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0malt_tf_idf_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0malt_tf_idf_summaries\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-2f319d859fda>\u001b[0m in \u001b[0;36malt_summarizer\u001b[0;34m(article, fully_tokenized, sentences, freq_dict, number)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0malt_summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfully_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt_tf_idf_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfully_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mordered_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0marticle_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfully_tokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minter_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-e91152570e9e>\u001b[0m in \u001b[0;36malt_tf_idf_scorer\u001b[0;34m(article, fully_tokenized, freq_dict)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malt_get_tf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfully_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-c79ef0693349>\u001b[0m in \u001b[0;36malt_get_tf_idf\u001b[0;34m(article, sentence, word, fully_tokenized, freq_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0malt_get_tf_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfully_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfully_tokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sentence26'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 05. Evaluation for all algorithms"
      ],
      "metadata": {
        "id": "n0Q6CGcTArbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our evaluation, we will use the ROUGE metric. *more info about rouge* We will use the Python library rouge to calculate the average score across all summaries."
      ],
      "metadata": {
        "id": "mUeO9ZUbA9pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### First-sentence evaluation"
      ],
      "metadata": {
        "id": "iJEsE_FoA2kJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files_rouge = FilesRouge()\n",
        "\n",
        "ref_path = '/content/highlights.csv'\n",
        "hyp_path = '/content/first_sentence.csv'\n",
        "scores = files_rouge.get_scores(hyp_path, ref_path, avg=True)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "3AcE88SaqMXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f5ac1c-b599-48c8-b3c1-2cc22f3dca20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.20554431682706864, 'p': 0.34040600685658673, 'f': 0.24848531754958594}, 'rouge-2': {'r': 0.07532355654981401, 'p': 0.12417890238162313, 'f': 0.09104541805334229}, 'rouge-l': {'r': 0.18619969323731397, 'p': 0.3095018892499339, 'f': 0.22542566250795346}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Word embeddings evaluation"
      ],
      "metadata": {
        "id": "9wsIDQVFBL6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ref_path = '/content/highlights.csv'\n",
        "hyp_path = '/content/word_embeddings.csv'\n",
        "scores = files_rouge.get_scores(hyp_path, ref_path, avg = True)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "cxh9nVuCBWyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c983d9eb-369d-4eee-a403-434764ac93dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.3122615692327619, 'p': 0.24340894037853636, 'f': 0.2621363264064126}, 'rouge-2': {'r': 0.09991188814894493, 'p': 0.07951737109637602, 'f': 0.08397359671211209}, 'rouge-l': {'r': 0.27641919476518373, 'p': 0.21590890958049447, 'f': 0.23226379623461724}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### TF-IDF evaluation"
      ],
      "metadata": {
        "id": "wOQtf9uMBS6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ref_path = '/content/highlights.csv'\n",
        "hyp_path = '/content/tf_idf.csv'\n",
        "scores = files_rouge.get_scores(hyp_path, ref_path, avg = True)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "k38nMeDlBWUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d975c8-b3ad-493b-be47-969c9aacdb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.23472882254313587, 'p': 0.1078323712775791, 'f': 0.14366650448554197}, 'rouge-2': {'r': 0.04632396033636655, 'p': 0.015411353810105565, 'f': 0.022531829820967745}, 'rouge-l': {'r': 0.220868608704233, 'p': 0.1016772326445643, 'f': 0.13540798067271909}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ALT TF-IDF (SENTENCES) Evaluation"
      ],
      "metadata": {
        "id": "7Y5ZlBJEQikq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ref_path = '/content/highlights.csv'\n",
        "hyp_path = '/content/alt_tf_idf.csv'\n",
        "scores = files_rouge.get_scores(hyp_path, ref_path, avg = True)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cS7pNXFQe7D",
        "outputId": "4f7c1e75-909b-44d5-f295-d5268b2d5cbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.24114747784123974, 'p': 0.10481348896749221, 'f': 0.14209125191410993}, 'rouge-2': {'r': 0.05148674904159248, 'p': 0.01609219449530112, 'f': 0.023945419244644838}, 'rouge-l': {'r': 0.22727144413772205, 'p': 0.09938519854765543, 'f': 0.1345063764191946}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u9csOSP2NYSp"
      }
    }
  ]
}