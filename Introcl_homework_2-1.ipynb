{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIN 353C: Introduction to Computational Linguistics,  Fall 2020, Erk\n",
    "\n",
    "# Homework 2: Loops, dictionaries, and language models\n",
    "\n",
    "## Due: Tuesday September 21, 3pm right before class\n",
    "\n",
    "## Your name: Veronica Alejandro\n",
    "## Your EID: vaa678\n",
    "\n",
    "This homework comes with the following files:\n",
    "\n",
    "* Introcl_homework_2.ipynb: this notebook, which has the homework problems. **Please put your answers into this same notebook.**\n",
    "\n",
    "\n",
    "Please record all your answers in the appropriate place in this notebook, and **do not forget to put your name and EID at the top of this notebook**.\n",
    "\n",
    "For the part of the homework that requires you to write Python code,\n",
    "we need to see the code.\n",
    "You can omit statements that\n",
    "produced an error or that did not form part of the eventual solution,\n",
    "but please include all the Python code that formed part of your\n",
    "solution. \n",
    "\n",
    "Please use comments to explain what your code does. Any code that seems complicated to you, or goes on for more than 2 lines, can probably use a comment. Just practice commenting more than you think the code needs. As you will see once you pull out an old piece of code you wrote and try to figure out what you were doing, code always needs more comments than you think.\n",
    "\n",
    "### Important note: Before you do anything else, please hit the fast-forward button on this notebook, and confirm \"Restart and Run all cells\", so the code included in this notebook will be executed on your machine.\n",
    "\n",
    "\n",
    "**If any of these instructions do not make sense to you, please get in\n",
    " touch with the instructor right away.**\n",
    "\n",
    "\n",
    "A perfect solution to this homework will be worth *100* points. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Punctuation, with loops (10 pts.) \n",
    "\n",
    "\n",
    "Here is a text passage from H.G. Wells, *The War of the Worlds* (from Project Gutenberg, http://www.gutenberg.org/etext/36):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellstext = \"\"\"After the glimpse I had had of the Martians emerging \n",
    "from the cylinder in which they had come to the earth \n",
    "from their planet, a kind of fascination paralysed my actions.  \n",
    "I remained standing knee-deep in the heather, staring at the \n",
    "mound that hid them.  I was a battleground of fear and curiosity.\n",
    "\n",
    "I did not dare to go back towards the pit, but I felt a passionate\n",
    "longing to peer into it.  I began walking, therefore, in a big curve,\n",
    "seeking some point of vantage and continually looking at the sand\n",
    "heaps that hid these new-comers to our earth.  Once a leash of thin\n",
    "black whips, like the arms of an octopus, flashed across the sunset\n",
    "and was immediately withdrawn, and afterwards a thin rod rose up,\n",
    "joint by joint, bearing at its apex a circular disk that spun with a\n",
    "wobbling motion.  What could be going on there?\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, please remove punctuation from beginnings and ends of words in the passage, but do not use regular expressions. Instead, please proceed as follows:\n",
    "\n",
    "* Please split the passage into words using Python's ```split()```. Do not use NLTK's ```word_tokenize()```, as that already does the work of removing punctuation for you. *(If there is already a tool to do this work for you, why should you practice how to do it by hand? For example because ```word_tokenize()``` may not work equally well for all languages, or because you may want to design a better ```word_tokenize()```.)*\n",
    "\n",
    "* Then run a for-loop over the list of words, and use Python's ```strip()``` method to remove punctuation. You will need to give `strip()` an appropriate parameter to do this. Collect the resulting list of words without punctuation in a separate list. (That is, use the \"aggregation\" programming idiom that we discussed in class.) You do not need to keep the punctuation in a separate string. For this problem please just You do not need to remove punctuation from the middle of strings, that is, you do not need to transform the string ``new-comers'' in any way. \n",
    "\n",
    "Here is information about the method ```strip()```: https://docs.python.org/3.8/library/stdtypes.html#str\n",
    "\n",
    "Also note that Python has a string full of punctuation symbols that might be useful for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the box for your punctuation removal code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'the', 'glimpse', 'I', 'had', 'had', 'of', 'the', 'Martians', 'emerging', 'from', 'the', 'cylinder', 'in', 'which', 'they', 'had', 'come', 'to', 'the', 'earth', 'from', 'their', 'planet', 'a', 'kind', 'of', 'fascination', 'paralysed', 'my', 'actions', 'I', 'remained', 'standing', 'knee-deep', 'in', 'the', 'heather', 'staring', 'at', 'the', 'mound', 'that', 'hid', 'them', 'I', 'was', 'a', 'battleground', 'of', 'fear', 'and', 'curiosity', 'I', 'did', 'not', 'dare', 'to', 'go', 'back', 'towards', 'the', 'pit', 'but', 'I', 'felt', 'a', 'passionate', 'longing', 'to', 'peer', 'into', 'it', 'I', 'began', 'walking', 'therefore', 'in', 'a', 'big', 'curve', 'seeking', 'some', 'point', 'of', 'vantage', 'and', 'continually', 'looking', 'at', 'the', 'sand', 'heaps', 'that', 'hid', 'these', 'new-comers', 'to', 'our', 'earth', 'Once', 'a', 'leash', 'of', 'thin', 'black', 'whips', 'like', 'the', 'arms', 'of', 'an', 'octopus', 'flashed', 'across', 'the', 'sunset', 'and', 'was', 'immediately', 'withdrawn', 'and', 'afterwards', 'a', 'thin', 'rod', 'rose', 'up', 'joint', 'by', 'joint', 'bearing', 'at', 'its', 'apex', 'a', 'circular', 'disk', 'that', 'spun', 'with', 'a', 'wobbling', 'motion', 'What', 'could', 'be', 'going', 'on', 'there']\n"
     ]
    }
   ],
   "source": [
    "# place your code here\n",
    "wellstextClean = wellstext.split()\n",
    "\n",
    "wordsNoPunct = []\n",
    "for word in wellstextClean:\n",
    "    new = word.strip(string.punctuation)\n",
    "    wordsNoPunct.append(new)\n",
    "\n",
    "print(wordsNoPunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Stemming, with loops (15 pts.)\n",
    "\n",
    "In the previous homework, you used regular expressions and word boundaries, \\\\b, for stemming. We now revisit stemming, but split the text into a word list first and then iterate over the members of the word list. We'll use the same text passage as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lewistext = \"\"\"And Peter with his sword still drawn in his hand went with \n",
    "the Lion to the eastern edge of the hill-top.  There a beautiful \n",
    "sight met their eyes.  The sun was setting behind their backs.  \n",
    "That meant that the whole country below them lay in the evening \n",
    "light--forest and hills and valleys and, winding away like a \n",
    "silver snake, the lower part of the great river.  And beyond \n",
    "all this, miles away, was the sea, and beyond\n",
    "the sea the sky, full of clouds which were just turning rose colour\n",
    "with the reflection of the sunset.  But just where the land of Narnia\n",
    "met the sea--in fact, at the mouth of the great river--there was\n",
    "something on a little hill, shining.  It was shining because it was a\n",
    "castle and of course the sunlight was reflected from all the windows\n",
    "which looked towards Peter and the sunset; but to Peter it looked like\n",
    "a great star resting on the seashore.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, please perform stemming on this text, as follows:\n",
    "    \n",
    "* Split the text into words, this time using ```nltk.word_tokenize()```. This will split off the punctuation into separate strings. The result is a list of words.\n",
    "\n",
    "* Iterate over the words in the list, perform stemming on each one (or leave it the same): Please remove word endings that are \"ing\", \"ed\", \"ion\", or \"s\" -- see options below. Put the resulting string in a new list. After the for-loop, the result should be a list of words exactly as long as the original word list, but stemmed. So this is another instance of the \"aggregation\" programming idiom we discussed in class.\n",
    "\n",
    "You have different options for how to identify word endings: You can use either ```endswith()```, or ```re.search()```. If you use ```re.search()```, note that the end of the word is now the end of the string, so you can use the anchor ```$``` instead of ```\\b```.\n",
    "\n",
    "You also have different options for removing endings: You can use string slices, or you can use ```re.sub()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['And', 'Peter', 'with', 'hi', 'sword', 'still', 'drawn', 'in', 'hi', 'hand', 'went', 'with', 'the', 'L', 'to', 'the', 'eastern', 'edge', 'of', 'the', 'hill-top', '.', 'There', 'a', 'beautiful', 'sight', 'met', 'their', 'eye', '.', 'The', 'sun', 'wa', 'sett', 'behind', 'their', 'back', '.', 'That', 'meant', 'that', 'the', 'whole', 'country', 'below', 'them', 'lay', 'in', 'the', 'even', 'light', '--', 'forest', 'and', 'hill', 'and', 'valley', 'and', ',', 'wind', 'away', 'like', 'a', 'silver', 'snake', ',', 'the', 'lower', 'part', 'of', 'the', 'great', 'river', '.', 'And', 'beyond', 'all', 'thi', ',', 'mile', 'away', ',', 'wa', 'the', 'sea', ',', 'and', 'beyond', 'the', 'sea', 'the', 'sky', ',', 'full', 'of', 'cloud', 'which', 'were', 'just', 'turn', 'rose', 'colour', 'with', 'the', 'reflect', 'of', 'the', 'sunset', '.', 'But', 'just', 'where', 'the', 'land', 'of', 'Narnia', 'met', 'the', 'sea', '--', 'in', 'fact', ',', 'at', 'the', 'mouth', 'of', 'the', 'great', 'river', '--', 'there', 'wa', 'someth', 'on', 'a', 'little', 'hill', ',', 'shin', '.', 'It', 'wa', 'shin', 'because', 'it', 'wa', 'a', 'castle', 'and', 'of', 'course', 'the', 'sunlight', 'wa', 'reflect', 'from', 'all', 'the', 'window', 'which', 'look', 'toward', 'Peter', 'and', 'the', 'sunset', ';', 'but', 'to', 'Peter', 'it', 'look', 'like', 'a', 'great', 'star', 'rest', 'on', 'the', 'seashore', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "lewistextWords = nltk.word_tokenize(lewistext)\n",
    "stemList = []\n",
    "\n",
    "for word in lewistextWords:\n",
    "    if word.endswith('ing'):\n",
    "        stem = word.replace('ing', '')\n",
    "        stemList.append(stem)\n",
    "    elif word.endswith('ed'):\n",
    "        stem = word.replace('ed', '')\n",
    "        stemList.append(stem)\n",
    "    elif word.endswith('ion'):\n",
    "        stem = word.replace('ion', '')\n",
    "        stemList.append(stem)\n",
    "    elif word.endswith('s'):\n",
    "        stem = word.replace('s', '')\n",
    "        stemList.append(stem)\n",
    "    else:\n",
    "        stemList.append(word)\n",
    "print(stemList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Building an n-gram language model (20 pts.)\n",
    "\n",
    "For this problem, you will build an n-gram language model. Use `MLE` from `nltk.lm`, as in the notebook \"n-gram language models with NLTK.\"\n",
    "\n",
    "Please proceed as follows:\n",
    "* Choose a particular book from which to build the language model: For this, go to Project Gutenberg, https://www.gutenberg.org/, and choose a book. Then download *the plain-text version* of the book.\n",
    "* Do the whole following pipeline twice, once for **bigram** language models and once for **trigram** language models. Each time, for n-grams of length n:\n",
    "  * Use padding, as appropriate for n-grams of length n.\n",
    "  * Compute \"everygrams\" up to lenght n, that is, use n-grams substituted with shorter n-grams.\n",
    "  * Train an nltk `MLE` language model.\n",
    "  * Use it to produce 100 words. (You can do this last step several times, and you will get new results each time. You can do this to explore your language model.)\n",
    "  \n",
    "In the text box below the code box, please comment on your result. How did the bigram model compare to the trigram model? How specific was the generated text to the style of the book you chose? Also please paste into your text answer your favorite text produced by your bigram, and your favorite text produced by your trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram model text:\n",
      "\n",
      "with a little shoeboy three minutes he ’ m going to go down again . really an instant like an ’ t you get at a large gold and her , ’ t stir , ” said Mr Cunningham said Mr O ’ t renege him looked older day to listen to Weathers came forward encouragingly towards one of the people were courting and I view my own behind the first he added Mr O , ” “ Papal infallibility of all turned suddenly . A spirit of the window I must have forgotten to you with great body of her , half full of a shilling to go out of the left at school . her mother ’ s no boy , how they leave. ” _ She felt that two minutes to the kitchen asking the corner and invited us saved up his mind was sober inartistic life . <s> At some words that he was over the conservatory and enthusiasm unyoke the health . life ) you done many years ago I went out , the groove of immorality ! laughter . ethical code his friends that didn ’ s heart . a sup taken Mary Jane . \n",
      "\n",
      "Trigram model text:\n",
      "\n",
      "before he had unexpectedly received some orders in advance ( he was about to knit his brows and , when he was like that idea very much moved . stirabout for fear I might give him an hour for your beautiful waltz , Miss Morkan. ” “ Browne is everywhere , ” said Mr O ’ Halloran stood a round and told lies about it if he intended to produce a spark from them , I can see him . passage over and over again : perfectly silent . well back on his own age and not come spying around here. ” “ And yet during all those great singers of the curates said he believed in the little pantry behind the stage she was very sensible of the coins . whom she had seen carrying cans and bottles to be polite , her husband and went over to O ’ Rourke I don ’ t you see ! we returned to his tea we watched the pair in view of certain other circumstances of the Board ladies had heard the story , waved the offer aside impatiently but Mr Bell , Miss Hill , please. ” She stopped suddenly as if he were a lady , while Mrs Conroy . for a small area of the window of a row against the sixpence in my grandmother ’ s the doctor never ordered anything of the side-tables were four very big barmbracks . . ” said Mr Holohan did not smile . "
     ]
    }
   ],
   "source": [
    "# space for your code here\n",
    "# BIGRAM CODE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "f = open('dubliners.txt', encoding = 'utf-8')\n",
    "dubliners = f.read()\n",
    "f.close()\n",
    "\n",
    "dubliners_sents = nltk.sent_tokenize(dubliners)\n",
    "dubliners_sents_tokenized = [nltk.word_tokenize(x) for x in dubliners_sents]\n",
    "\n",
    "bigram = 2\n",
    "dubliners_corpus, vocabulary = padded_everygram_pipeline(bigram, dubliners_sents_tokenized)\n",
    "\n",
    "dubliners_model = MLE(bigram)\n",
    "dubliners_model.fit(dubliners_corpus, vocabulary)\n",
    "\n",
    "print('Bigram model text:')\n",
    "print()\n",
    "for sentno in range(10):\n",
    "    words = dubliners_model.generate(100)\n",
    "    for w in words:\n",
    "        # If the word is \"sentence end\", stop generating\n",
    "        # and move on to the next sentence.\n",
    "        # 'break' will end the current (innermost) \n",
    "        # for-loop immediately.\n",
    "        if w == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            # We're not at sentence end yet.\n",
    "            # Print the current word.\n",
    "            print(w, end = \" \")\n",
    "            \n",
    "# TRIGRAM CODE\n",
    "# ntf\n",
    "print('\\n')\n",
    "print('Trigram model text:')\n",
    "print()\n",
    "trigram = 3\n",
    "dubliners_corpus2, vocabulary = padded_everygram_pipeline(trigram, dubliners_sents_tokenized)\n",
    "dubliners_model2 = MLE(trigram)\n",
    "dubliners_model2.fit(dubliners_corpus2, vocabulary)\n",
    "\n",
    "for sentno in range(10):\n",
    "    words = dubliners_model2.generate(100)\n",
    "    for w in words:\n",
    "        # If the word is \"sentence end\", stop generating\n",
    "        # and move on to the next sentence.\n",
    "        # 'break' will end the current (innermost) \n",
    "        # for-loop immediately.\n",
    "        if w == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            # We're not at sentence end yet.\n",
    "            # Print the current word.\n",
    "            print(w, end = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How did the bigram model compare to the trigram model?** <br>\n",
    "I definitely think the trigram model made bit more sense, though both are pretty incomprehensible. I noticed that the punctuation for the trigram was slightly better, as well as connecting words. <br>\n",
    "<br>\n",
    " \n",
    "**How specific was the generated text to the style of the book you chose?** <br>\n",
    "I think the style wasn't overtly present in the generated text, but due to the nature of the words from the book, there was some clear word choices that seemed more like the style of the book. Other than that just word choices, there isn't much *style* throughout the generated texts. I think there were a few phrases I liked that seemed really poetic, though. For example, for one of the generated texts I got \"dusky golden light\" which seemed really pretty to me. But this could also just be straight up exact from the book, because I noticed times when that was the case. <br>\n",
    "<br>\n",
    "**Also please paste into your text answer your favorite text produced by your bigram, and your favorite text produced by your trigram.** <br>\n",
    "This was one of the funniest paragraphs I got, which is why its one of my favorites for the trigram text. There are some funny phrases like \"the Irish Revival began to speak to them,\" or\" \"showing his biceps muscle to the banisters.\" <br>\n",
    "<br>\n",
    "not to understand . too glad .... You wait a minute or so the taller . , and Gabriel was again in dear dirty Dublin .... When the Irish Revival began to speak to them , no rapture . changed to peony . and large .... Mr Cotter ? ” said Corley . loved her ? showing his biceps muscle to the banisters to listen to the criticisms of an old stage-wardrobe and the pallor of the celibate , the person long ago and no damn good : that was spread about his speech in silence until their drinks . head and a neatly groomed young man with no hunker-sliding about him . topped with grated nutmeg , a few minutes . <br>\n",
    "<br>\n",
    "For the bigram text, it was difficult to choose a favorite one because they were all kind of bad and rarely even comprehensible. I would choose the following one because there were some funny lines, like \"what would have people were more real cheese\". <br>\n",
    "<br>\n",
    "Henchy waited until he undid his fist on his hands with a sweetheart and saw such a dinged silk hat had fallen on his arm , ” said : what would have people were more real cheese , ” asked Gabriel , Gretta ? ” said nothing very nice and walked rapidly and Aunt Kate said my aunt ’ s no play every ridiculous detail of the gentlemen sat downstairs , though she never heard often to help him ? ” _ “ The Isle of him had happened morning of the result pricked him .... Julia had approached the grease on the main PG search of how she would pull the card . <br>\n",
    "<br>\n",
    "The pattern with choosing my favorites is basically choosing which one makes me laugh the most because these are nearly incomprehensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General information for all the following problems\n",
    "\n",
    "## Part-of-speech tags, POS tags for short\n",
    "\n",
    "Part-of-speech tagging means labeling words in a text as \"this is a noun\", \"this is a verb\", \"this is a preposition\", and so on. There are some texts that have been manually labeled (by human annotators) with POS-tags, among them the Brown corpus (see below). Automatic POS-tagging is an important NLP task. \n",
    "\n",
    "Usually, the POS-tags we see used are more fine-grained than just \"this is a verb\" versus \"this is a noun\", they may distinguish, for example, nouns in singular and plural (\"house\" versus \"houses\"), and verbs in base form, 3rd person singular, and past tense (\"stroll\", \"strolls\", \"strolled\"). \n",
    "\n",
    "Here is a list of the POS-tags we will use in the following homework problems: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used\n",
    "\n",
    "## Accessing the POS-tagged Brown corpus\n",
    "\n",
    "The Brown corpus is a collection of text pieces from different domains, selected to be about balanced in size across different text types. It comprises about 1 million words. Here is more information on the Brown corpus: https://en.wikipedia.org/wiki/Brown_Corpus\n",
    "\n",
    "In this and the following problems\n",
    "you will use the *news* part of the Brown Corpus in the Natural\n",
    "Language Toolkit, more precisely a version of this corpus with\n",
    "part-of-speech tags (POS tags). In order to do that, you will need to\n",
    "install the NLTK data (if you have not done this already). Do this as follows:\n",
    "\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "Then follow the further instructions at http://www.nltk.org/data\n",
    "\n",
    "Afterwards, you can access the POS-tagged *news* part of the Brown Corpus as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_news = nltk.corpus.brown.tagged_words(categories=\"news\")\n",
    "\n",
    "brown_news[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you a list of tuples, to  be more precise a list of\n",
    "pairs. The first member of each pair is a word, and the second member\n",
    "is its part-of-speech tag. \n",
    "\n",
    "A tuple in Python is almost the same as a list: It is a sequence of\n",
    "items, and it is written like a list, except with round brackets instead of straight brackets. You can access an item using its index, as in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AT'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytuple = ('The', 'AT')\n",
    "mytuple[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one difference is that you cannot change a tuple, so for example\n",
    "you cannot use ```append()``` on it. \n",
    "\n",
    "For all problems below, please map all words to lowercase in order not to distinguish \"The\" and \"the\". You can use the method ```lower()``` for this, as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"The\".lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Word frequency and tag frequency, with Python dictionaries (30 pts.)\n",
    "\n",
    " What is the most frequent word, and what is the most frequent POS-tag in the *news* section of Brown? \n",
    " \n",
    " Do *not* use the NLTK data structures ```nltk.FreqDist```\n",
    "and ```nltk.ConditionalFreqDist```, or the Python data type\n",
    "```Counter```, for this problem. Just use Python dictionaries,\n",
    "  along with the built-in function ```sorted()```:\n",
    "  \n",
    "  * Make a dictionary to keep counts of words. \n",
    "  * Iterate through the list of word/tag pairs. Extract the word from the pair, and use a Python dictionary to keep count of how often you meet each word.\n",
    "  * Now make a dictionary to keep counts of tags. Iterate through the list of word/tag pairs again, but this time extract the tags and count them.\n",
    "  * Use ```sorted()``` to sort each dictionary by its values, and find the entries with the highest values.\n",
    "  \n",
    "**New:**\n",
    "\n",
    "* What is the count for the word \"the\"?\n",
    "* What is the count for the tag \"AT\"?\n",
    "\n",
    "This is similar to the exercise we did in class where, for each\n",
    "word appearing in a paragraph from *The Onion*, we counted how often\n",
    "it appeared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT FOR THE WORD 'the':\n",
      "6386\n",
      "\n",
      "COUNT FOR THE TAG 'AT':\n",
      "8893\n"
     ]
    }
   ],
   "source": [
    "wordDict = {}\n",
    "tagDict = {}\n",
    "\n",
    "for pair in brown_news:\n",
    "    word = pair[0].lower()\n",
    "    if word not in wordDict:\n",
    "        wordDict[word] = 0\n",
    "    wordDict[word] += 1\n",
    "\n",
    "for pair in brown_news:\n",
    "    tag = pair[1]\n",
    "    if tag not in tagDict:\n",
    "        tagDict[tag] = 0\n",
    "    tagDict[tag] += 1\n",
    "\n",
    "print(\"COUNT FOR THE WORD 'the':\")\n",
    "print(wordDict['the'])\n",
    "print()\n",
    "print(\"COUNT FOR THE TAG 'AT':\")\n",
    "print(tagDict['AT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5: Word frequency and tag frequency, with NLTK's FreqDist (10 pts.)\n",
    "\n",
    "Please re-do the task from problem 4, but this time use the NLTK ```FreqDist``` data type: What is the most frequent word, and what is the most frequent POS-tag in the *news* section of Brown? This time, create one ```nltk.FreqDist``` object for counts of words, and one for counts of tags. Then use the FreqDist method ```max()``` to determine the most frequent word and the most frequent tag. (Please make sure you get the same answer as before.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent word is: the\n",
      "The most frequent tag is: NN\n"
     ]
    }
   ],
   "source": [
    "# please place your code here\n",
    "brownWords = [item[0] for item in brown_news]\n",
    "brownTags = [item[1] for item in brown_news]\n",
    "\n",
    "freqWords = nltk.FreqDist(brownWords)\n",
    "freqTags = nltk.FreqDist(brownTags)\n",
    "\n",
    "print('The most frequent word is: {}'.format(freqWords.max()))\n",
    "print('The most frequent tag is: {}'.format(freqTags.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6: Ambiguous words, with Python dictionaries (15 pts.)\n",
    "\n",
    "In this problem, you will use nltk's ``ConditionalFreqDist`` to determine which words in the Brown news corpus have multiple different tags: How many words are ambiguous, in the sense that they appear with at least two different POS tags? For example, the English word \"object\" is ambiguous in that it could be either a verb or a noun (though I do not know if it occurs in the Brown news corpus as both a verb and a noun). \n",
    " \n",
    "To solve this problem, build an nltk ``ConditionalFreqDist`` from word/tag pairs *again only using the news part of the Brown corpus*.  You can see an example of a `ConditionalFreqDist` object in the notebook on n-gram language models. Here is another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 3 conditions>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "pairs = [ (\"a\", \"X\"), (\"a\", \"Y\"), (\"b\", \"X\"), (\"c\", \"Y\")]\n",
    "\n",
    "# this makes a ConditionalFreqDist object\n",
    "cd = nltk.ConditionalFreqDist(pairs)\n",
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the different values seen as first of a pair are conditions\n",
    "cd.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'X': 1, 'Y': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the entry for \"a\" is all the second-of-a-pair entries seen with \"a\".\n",
    "# It is a FreqDist object\n",
    "cd[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Y \n",
      "1 1 \n"
     ]
    }
   ],
   "source": [
    "# So we have a complex data type: \n",
    "# A ConditionalFreqDist contains many FreqDist objects,\n",
    "# one for each condition.\n",
    "# You can save the FreqDist for \"a\" in a variable.\n",
    "fda = cd[\"a\"]\n",
    "# Then all the methods that come with a FreqDist\n",
    "# are available with that object, for example\n",
    "# the method tabulate(), which pretty-prints\n",
    "# all the counts. (This is not the method you need,\n",
    "# it is just a demonstration of some FreqDist method.)\n",
    "fda.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'Grand', 'said', 'of', 'primary', 'produced', 'no', 'that', 'place', '.', 'jury', 'further', 'in', 'the', 'City', 'Executive', 'Committee', ',', 'had', 'charge', 'and', 'thanks', 'Atlanta', 'for', 'was', 'conducted', 'charged', 'by', 'Court', 'to', 'reports', 'won', 'Only', 'a', 'such', 'received', 'considering', 'number', 'size', 'this', 'it', 'are', 'recommended', 'act', 'studied', 'revised', 'end', 'commented', 'on', 'purchasing', 'well', 'operated', 'accepted', 'practices', 'best', 'proposed', 'However', 'be', 'combined', 'cost', 'is', 'lacking', 'as', 'result', 'urged', 'problem', 'next', 'funds', 'date', 'so', 'at', 'State', 'handling', 'welfare', 'one', 'major', 'general', 'program', 'but', 'fit', 'through', 'all', 'state', 'with', 'might', 'less', 'feel', 'future', 'some', 'do', 'will', 'court', 'under', 'fees', 'found', 'into', 'effect', 'from', 'costs', 'appointed', 'elected', 'new', 'Jan.', '1', 'not', 'added', 'there', 'pricing', 'jail', 'deputies', ':', '(', ')', 'Four', '2', 'work', 'pass', 'permit', 'fair', 'plan', 'Police', 'Grady', 'Hospital', 'Mayor', 'William', 'B.', 'filed', 'divorce', 'his', 'son', 'Mrs.', 'J.', 'agreed', 'upon', 'listed', 'Ala.', 'lived', 'more', 'than', 'year', 'home', 'E.', 'since', 'back', 'present', 'succeeded', 'after', 'announced', 'would', 'run', 'Georgia', 'Republicans', 'race', 'top', 'official', 'GOP', 'meeting', 'held', 'Blue', 'brought', 'Party', 'James', 'W.', 'up', '8', 'newly', 'Texas', 'John', 'In', 'warned', 'force', 'petitions', 'out', 'warning', 'vote', 'attended', 'asked', 'wanted', 'wait', 'face', 'before', 'making', 'first', 'alternative', 'must', 'Five', 'county', 'sign', 'allowed', 'hold', 'party', 'public', 'relations', 'campaign', 'expected', 'assistant', 'three', 'years', 'starts', 'become', 'wind', 'head', 'approved', 'Before', 'study', 'allotted', 'made', 'visit', 'likely', 'issue', 'earlier', 'Construction', 'bonds', 'learned', 'very', 'near', 'worth', 'go', 'test', 'contracts', 'let', 'most', 'heavily', 'traveled', 'A', 'revolving', 'fund', 'issued', 'paid', 'off', 'tax', 'opened', 'against', 'roads', 'told', 'consulted', 'yet', 'about', 'plans', 'offer', 'House', 'increase', 'research', 'outright', 'notice', 'sought', 'worked', 'set', 'passed', 'As', 'offered', 'last', 'rejected', 'pay', 'veteran', 'Jackson', 'aid', 'opposed', 'past', 'Barber', 'increased', 'support', 'provided', 'Washington', 'like', 'read', 'Congress', 'Board', 'Education', 'directed', 'Colquitt', 'After', 'long', 'Miller', 'put', 'I', 'saw', 'defeated', 'principal', 'Democratic', 'votes', 'got', 'Carey', 'order', 'just', 'church', 'drop', 'bit', 'calls', 'P.', 'shot', 'March', '18', 'days', 'post', 'too', 'scheduled', 'Many', 'local', 'ordinary', 'good', 'promise', 'real', 'There', 'Austin', 'Price', 'abandoned', 'led', 'fight', 'measure', 'down', 'considerably', 'hearing', 'Revenue', 'Under', 'rules', 'week', 'left', 'little', 'doubt', 'termed', 'extremely', 'estimate', '17', 'help', 'merely', 'means', 'permits', 'over', 'stocks', 'seven', 'report', 'because', 'almost', 'declared', 'Lawrence', 'keynote', 'you', 'Charles', 'Hughes', 'Sherman', 'sponsor', 'amount', 'denied', 'Jones', 'Howard', 'argued', 'enough', 'introduced', 'only', 'Senators', 'Dallas', 'schools', 'special', 'schooling', 'reduced', 'attend', 'here', 'five', 'Harris', 'estimated', 'live', 'Red', 'still', 'pro', 'betting', 'better', 'informed', 'question', 'San', 'heard', 'proposal', 'faces', 'hard', 'later', 'house', 'sent', 'planning', 'senate', 'One', 'Beaumont', 'third', 'Lamar', 'adopted', 'R.', 'Two', 'Louis', 'return', 'fine', 'Research', 'Paris', 'own', 'license', 'gas', 'right', '3', 'underground', 'Marshall', 'suggested', 'need', 'selected', 'chief', 'decided', 'taking', 'kind', 'kept', 'April', '4', 'U.S.', 'delay', 'West', 'reported', 'Feb.', '22', 'two-thirds', 'All', 'lost', 'water', 'needs', 'big', 'Fort', 'cover', 'places', 'ended', 'sponsored', 'Most', 'maximum', 'project', 'felt', 'spent', 'providing', 'benefit', 'attack', 'De', 'La', 'serving', 'learning', 'C.', '24', '12', 'teaching', 'required', 'junior', 'senior', 'high', 'credit', 'English', 'permitted', 'College', 'joined', 'named', 'Dr.', 'close', 'earned', 'Oklahoma', '&', 'Southwestern', 'Massachusetts', 'served', 'High', 'associate', 'Denton', 'Chester', 'O.', 'called', 'Cook', 'New', 'firm', '10', 'range', 'case', 'causes', 'Must', 'solve', 'We', 'forced', 'cases', 'deal', 'limited', 'extension', 'Other', 'living', 'police', 'trial', 'viewed', 'grant', 'request', 'while', 'prejudicial', 'aspects', 'Some', 'indicated', 'others', 'honor', 'amounts', 'conflict', 'July', 'President', 'Kennedy', 'pushed', 'White', 'business', 'Berlin', 'tomorrow', 'American', 'much', 'writing', 'talked', 'Chief', 'returned', 'quite', 'way', 'Asked', 'Pierre', 'press', \"it's\", 'staff', 'involved', 'wording', 'rather', 'meet', 'Acting', 'obtained', 'matter', 'even', 'ruled', '133', 'Morris', 'admitted', 'questioned', 'building', 'constituted', 'used', 'outside', 'judge', 'East', 'St.', '24th', 'precedent', 'reading', '9', 'care', 'raised', 'covered', 'tied', 'requests', 'finance', 'hill', '$37', 'boost', 'rise', 'figures', 'higher', 'payment', 'patient', 'nursing', 'following', 'use', 'noted', 'modest', 'wish', 'staggered', 'caused', 'stay', 'cut', 'essential', 'doctors', 'needed', 'equal', 'matching', 'More', 'field', 'institute', 'how', 'For', 'Congressional', 'along', 'lines', 'Legislators', 'criticized', 'hailed', 'Republican', 'Ill.', 'change', 'Tex.', 'mighty', 'housing', '11', 'speed', 'rule', 'demand', 'Wayne', 'Oslo', 'North', 'Atlantic', 'Foreign', 'understanding', 'route', 'step', 'met', 'particularly', 'powers', 'Africa', 'allies', 'troubles', 'walk', 'Council', 'General', 'discussed', 'advance', 'Canada', 'somewhat', 'behind', 'daily', 'arms', 'closer', 'United', 'hoped', 'Dean', 'clear', 'mood', 'Soviet', 'individual', 'march', 'inside', 'stand', 'once', 'again', 'whole', 'surveyed', 'around', 'treated', 'regard', 'lack', 'main', 'strengthening', 'forces', 'come', 'laid', 'completely', 'fulfilled', 'moves', 'Geneva', 'Laos', 'military', 'setting', 'international', 'Viet', 'Nam', 'count', 'hopes', 'Lao', 'Policies', 'modified', 'control', 'showing', 'gain', 'How', 'Detroit', 'tougher', 'restrained', 'hardly', 'raising', 'Cuba', 'score', 'reply', 'Morton', 'National', 'Boston', 'groups', 'trade', 'Latin', 'America', 'Until', 'Cuban', 'voiced', 'revamped', 'early', 'disclosed', 'Eisenhower', 'reviewed', 'changed', 'thus', 'far', 'looked', 'show', 'Asian', 'faster', 'rate', 'And', 'exposed', 'acclaimed', 'Asia', 'steamed', 'prepared', 'risk', 'war', 'concluded', 'ill', 'favor', 'Royal', '25', 'erred', 'half', 'encouraging', 'check', 'juvenile', 'deeply', 'Crime', 'supported', 'recognized', 'Attorney', 'Labor', 'assist', 'David', 'sense', 'doubled', 'Federal', 'Providence', 'handed', 'believed', 'men', 'Along', 'headquarters', 'rescue', 'bet', 'know', 'point', 'Such', 'call', 'developed', 'name', 'move', 'form', 'clerk', 'released', 'complete', 'review', 'Gen.', 'views', 'advised', 'across', 'limit', 'sold', 'concern', 'especially', 'placed', 'retained', 'Rotary', 'average', 'pull', 'Socialist', 'That', 'retired', 'gross', 'neither', 'wage', 'caught', 'profit', 'turned', 'really', 'counseling', 'Johnston', 'presented', 'hand', 'explained', 'handled', 'assured', 'correct', 'hope', 'surprised', 'Our', 'followed', 'started', 'speaking', 'model', 'meaning', 'cited', 'included', 'Action', 'minor', 'traffic', 'draft', 'authorized', 'Nothing', 'Local', 'Plainfield', 'value', 'Westfield', 'Young', 'greeted', 'Trenton', 'stopped', \"he's\", 'sell', 'remains', 'considered', 'fall', 'common', 'either', 'open', 'launched', 'Jersey', 'industry', 'share', 'blame', 'plant', 'until', 'saved', 'faced', 'vehicles', 'challenge', 'stands', 'Ike', 'position', 'ahead', 'May', 'gathering', 'Case', 'answer', 'failed', 'glow', 'purchase', 'Appeals', 'filled', 'Conservation', 'Commissioner', 'credits', 'Michigan', 'established', 'expanded', 'training', 'credited', 'co-operative', 'bid', 'nearly', 'expedient', 'win', 'planned', 'talk', 'full', 'stop', 'To', 'lives', 'representative', 'Green', 'land', 'green', 'comment', 'honored', 'Wagner', 'Screvane', 'Beame', 'regarded', 'Brooklyn', '7', 'file', 'view', 'Buckley', 'visits', 'touch', 'replaced', 'Last', 'visited', 'key', 'talks', 'assumed', 'Hunter', 'June', '16', 'feeling', 'remarks', \"It's\", 'influence', 'object', 'identified', 'novel', 'works', 'escape', 'Day', 'Public', 'accused', 'hopeful', 'encounter', 'low', 'Thomas', 'Three', 'carried', 'corps', 'submitted', 'stage', 'Six', 'Question', 'discovered', 'worry', 'U.', 'S.', 'Government', 'explains', \"don't\", 'No', 'Moscow', 'pulled', '!', 'allocated', 'beginning', '1954', 'Nations', 'N.', 'Y.', 'filling', 'due', 'officer', 'start', 'above', 'joint', 'Britain', 'round', 'pace', 'slowed', 'reached', 'principles', 'search', 'commercial', 'artist', 'advertising', 'native', 'Orleans', 'Field', 'Miles', 'Pennsylvania', 'parade', 'mass', 'Hill', 'ceremonies', 'ride', 'impressive', 'approach', 'buildings', 'Of', 'block', 'Great', 'recorded', 'watch', 'square', 'Street', 'car', 'opposite', 'Battle', '40', 'Division', 'Then', 'headed', 'Force', 'Miss.', 'largely', 'surrounded', 'scramble', 'iron', 'throw', 'honeymoon', 'Smith', 'equally', 'clouded', 'ranks', 'bypass', 'flow', 'remained', 'La.', 'related', 'cash', 'weary', 'Indeed', 'clearing', 'play', 'toss', 'frustrated', 'though', 'continued', 'Since', 'spending', 'blocked', 'trim', 'tried', 'bids', 'ranged', 'sued', 'contract', 'Philadelphia', 'played', 'appeared', 'fired', 'charges', 'sue', 'bankrupt', '2d', 'draw', 'Business', 'claim', 'violated', 'Pratt', '48', 'wide', 'raise', 'houses', 'single', 'account', '95', 'Home', 'Virginia', 'flat', 'owners', 'killed', 'Warren', 'K.', 'drive', 'driving', 'Lee', 'Eastwick', 'parks', 'Residential', 'master', 'feature', 'second', 'Would', 'Kansas', 'Mo.', 'UPI', 'injured', 'figure', 'teamsters', 'suffered', 'Baptist', 'blast', \"I'd\", 'flash', 'boy', 'knocked', 'Hood', 'flies', 'reward', 'Turkey', 'Oct.', 'AP', 'contested', '15', 'bargaining', 'threat', 'demanded', 'created', 'practicing', 'Portland', 'graduate', 'structure', 'Multnomah', 'Germany', 'rebel', 'Individual', 'fear', 'total', 'SW', 'thought', 'Los', 'Angeles', 'her', 'rush', 'Salem', 'greeting', 'Circuit', 'Beaverton', 'advisory', 'Temple', 'organized', 'Roosevelt', 'Oregon', 'Al', 'Oak', 'Grove', 'Lodge', 'Barbara', 'God', 'compromise', 'editing', 'Old', 'Christ', 'Election', 'opening', 'stressed', 'Church', 'light', 'Illinois', 'England', 'meets', 'pioneer', 'centers', 'graduates', 'striking', 'upheld', 'Miami', 'Fla.', 'Orioles', 'dropped', 'straight', '5', 'late', 'Siebern', 'hits', 'homer', 'Over', 'entered', 'deadlock', 'throws', 'wild', \"House's\", 'Rookie', 'tally', 'performed', 'pitchers', 'doubles', 'Robinson', 'Hartman', 'double', 'deep', 'Ward', 'short', 'Lumpe', 'moved', 'beat', 'error', 'stretch', 'scoring', 'hit', 'fast', 'break', 'Yankees', 'Florida', 'news', 'slugged', 'Baltimore', 'signed', 'pounds', 'lighter', 'checked', 'camp', 'improved', 'fielding', 'Brown', 'champions', 'Duren', 'Sheldon', 'posted', 'Art', 'Yankee', 'Petersburg', 'assisting', 'pretty', 'pitching', 'arrived', 'delayed', 'Md.', '8,280', 'races', 'lead', 'tired', 'condition', 'crossing', 'gets', 'Self', 'Cleveland', 'Columbus', 'Mills', 'Big', 'turn', 'timed', 'finished', 'Ohio', 'Francisco', 'Franklin', 'donated', 'Chicago', 'Sox', 'plays', 'Time', 'kick', 'hip', 'favored', 'Cotton', 'kicked', '14', 'tries', 'Stamford', 'So', 'kicking', 'barely', '26', '77', 'Place', 'timing', 'Once', 'scored', 'Coach', 'kids', 'stayed', 'Happy', 'loose', 'contributed', 'spotted', 'lay', 'split', 'Maryland', \"He's\", 'Held', 'tackle', 'treatment', 'Because', 'End', 'Trinity', 'Denver', 'Louisiana', 'exceptionally', 'passes', 'delivered', 'claimed', 'resulted', 'checks', 'ordered', 'tests', 'pack', 'walked', 'lagged', 'Indians', 'tripled', 'Cliff', 'Lane', 'Mich.', 'ranked', 'Minnesota', 'batted', 'pop', 'fly', 'sacrifice', 'aboard', 'Ogden', 'Utah', 'train', 'Pacific', 'warmed', 'quoted', 'Vernon', 'Slaughter', 'merits', 'hitting', 'TV', 'Cincinnati', \"Mantle's\", 'Post', 'slashed', 'accomplished', '23', 'refused', 'Pittsburgh', 'Catholic', 'silver', 'award', 'Baseball', 'Ben', 'Guy', 'Babe', 'Ruth', 'Mercer', 'Show', 'follows', 'Gimbel', 'California', 'Flushing', 'stadium', 'Ford', 'Point', 'owned', 'Long', 'Roger', 'amateur', 'Gold', 'Hogan', 'Missouri', 'golden', 'achieved', 'human', 'shooting', 'margin', 'slice', '19', 'Chisholm', 'picked', 'alongside', 'spot', 'happened', 'strings', 'stroke', 'supply', 'Warwick', 'Harvey', 'Nieman', 'lineup', 'bats', 'honors', 'Billiken', 'Notre', 'Dame', 'overcome', \"that's\", 'meant', \"Yankees'\", 'inherited', 'C', 'Major', 'spread', 'Alvin', 'rushed', 'marriage', 'mother', 'Met', 'Cancer', 'Arms', 'Stevens', 'Here', 'Fashion', 'Armed', 'First', 'Honolulu', 'London', 'look', 'Debutante', \"Chicago's\", 'Italian', 'Europe', 'King', 'Phoenix', 'Pretty', 'Ball', 'Jane', 'singing', 'cry', 'wed', 'mail', 'toast', 'Back', 'Flower', 'People', 'touches', 'Fall', 'Best', 'Stock', 'Mets', 'uniform', 'Nobel', 'African', 'rest', 'swim', 'import', 'closed', 'gala', 'Music', 'dancing', 'coast', 'Santa', 'Colorado', 'dining', 'Guests', 'staged', 'preceded', 'Center', 'bridge', 'pressing', 'Week', 'plenty', 'welcome', 'wants', 'magic', 'pools', 'marvel', 'motel', '$14', 'gold', 'white', 'Methodist', 'Coronado', 'Perkins', 'Presbyterian', 'Rhodes', 'Hartford', 'seated', 'Christian', 'Philmont', 'exactly', 'Basin', 'effects', 'display', 'stock', 'Walnut', 'throughout', 'totals', 'Besides', 'Others', 'Anderson', 'entertained', 'Drexel', 'Vienna', 'Anne', 'Taylor', 'Easter', 'Wall', 'celebrated', 'graduated', 'Wellesley', 'Lady', 'Clayton', 'dances', 'edging', 'cuts', 'lay-offs', 'decline', 'struck', 'register', 'damaged', 'Arundel', 'Annapolis', 'discrepancies', 'contained', 'summoned', 'dispatched', 'longer', 'Werner', 'resumed', 'supplies', 'trained', 'Chesapeake', 'broadcast', 'brothers', 'Lloyd', 'Simpkins', 'Somerset', 'compared', 'Dodge', 'enjoyed', 'manufacturing', 'Convenience', 'increasingly', 'predicted', 'testified', 'British', 'devoted', 'Russian', 'arrested', 'roll', 'Lola', 'marks', 'drawing', 'bail', 'Bailey', 'criminal', 'German', 'Russia', 'Fourteen', 'selling', 'highest', 'underwater', 'potential', 'cleared', 'watched', 'designing', 'advanced', \"Navy's\", 'branch', 'Westinghouse', 'Electric', 'detail', 'narcotic', 'peddlers', 'posed', 'comments', 'witness', 'Stewart', '51st', 'Morgan', 'bound', 'express', 'accosted', 'Blanche', 'Patrolman', 'Jenks', '54', 'Still', 'Services', 'Funeral', 'cook', 'Eight', 'Road', 'candle', 'Mother', 'collected', 'Students', 'Dequindre', 'Principal', 'Student', 'McClellan', 'pursued', 'Traffic', 'Murphy', 'thinking', 'note', 'appeal', 'Havana', 'plot', 'Report', 'swept', 'Emory', 'intend', 'exempt', 'existence', 'intellectual', 'moral', 'NE', 'Leon', 'renewed', 'Human', 'contacted', 'Black', 'pledged', 'Killingsworth', 'Greenville', 'Little', 'attacked', 'Hall', 'Macon', 'Turner', 'Northwest', 'flames', 'Murray', '50th', 'Fair', 'Wilson', 'counseled', 'investment', 'Madison', 'Hillsboro', 'shows', 'exhibited', 'male', 'slightly', 'ransacked', 'French', 'Eddy', 'dumping', 'bought', 'strike', 'hired', 'Beverly', 'rear', 'acted', 'finding', 'represented', 'Aurora', 'Birmingham', 'Neither', 'masked', 'formal', 'spraying', 'controls', 'pride', 'breed', 'halt', 'Marin', 'installed', 'rolled', 'shipping', 'strip', 'controlled', 'crack', 'Glimco', 'buddy', 'payments', '$12,500', 'lists', 'Bonn', 'Foods', 'Investors', 'Guerin', 'offering', 'yield', 'Second', 'acting', 'Headquarters', 'Hardwicke-Etter', 'Moss', 'Gordin', 'harvesting', 'overhead', 'standard', 'gains', 'Page', 'complex', 'lift', 'pan', 'refuted', 'Rather', 'routine', 'lower', 'traded', 'dull', 'Japanese', 'exchange', 'convertible', '1986', 'B', 'rose', 'buying', \"1960's\", 'invited', 'feed', 'returns', 'incurred', 'entitled', 'applied', 'realized', 'Neiman-Marcus', 'Are', 'Morocco', 'baking', 'brown', 'truly', 'finish', 'pool', 'Wells', 'Hilton', 'heads', 'matched', 'Swim', 'Wilshire', 'Budapest', 'addresses', 'Stars', 'Flowers', 'Pasadena', 'Changes', 'flexible', 'delight', 'sufficiently', 'appealing', 'Fike', 'features', 'Heilman', 'Lauderdale', 'Theatre', 'Ballet', 'Coral', 'Hollywood', 'Library', 'convinced', 'Kirov', 'ballet', 'mine', 'totally', 'fixing', 'Toys', '16th', 'moderates', 'stuck', 'calm', 'match', 'Brevard', 'noticed', 'impressed', 'discouraged', 'crucial', 'implement', 'lull', 'confronted', 'China', 'Control', 'wonder', 'brand', 'academic', 'declines', 'gamble', 'substitute', 'allegedly', 'Congo', 'swung', 'Said', 'ante', 'presidency', 'Player', 'Augusta', 'putt', 'birdie', 'birdied', 'boldness', 'borrowed', 'surprising', 'dug', 'conspired', 'U.N.', 'undergraduate', 'interviewed', 'trusted', 'wildly', 'Belgian', 'administered', 'touched', 'Congolese', 'detested', 'Kasai', 'Progress', 'Boeing', '707', 'hijacked', 'radioed', 'access', 'textile', 'Wales', 'Nassau']\n"
     ]
    }
   ],
   "source": [
    "# Space for your code to solve the problem\n",
    "pairsCond = nltk.ConditionalFreqDist(brown_news)\n",
    "conditions = pairsCond.conditions()\n",
    "\n",
    "ambi = []\n",
    "\n",
    "for cond in conditions:\n",
    "    freqDist = pairsCond[cond]\n",
    "    count = len(freqDist)\n",
    "    if count > 1:\n",
    "        ambi.append(cond)\n",
    "\n",
    "print(ambi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble solving this one, read on below.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "To solve this problem, proceed as follows:\n",
    "* Make a ConditionalFreqDist from all the word/tag pairs in the Brown news corpus\n",
    "* Then use the aggregator programming idiom: \n",
    "  * Make an empty variable to store all ambiguous words\n",
    "  * Iterate over all the conditions in the ConditionalFreqDist (conditions are words here)\n",
    "  * For each condition/word, retrieve its associated FreqDist.\n",
    "  * Determine how many different tags there are that have counts in this FreqDist.\n",
    "    If you need help remembering which function to use for this, run ```help(nltk.FreqDist)``` in a new code box.\n",
    "  * if there is more than one tag that has counts in this FreqDist, store this condition/word in your variable that holds all ambiguous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
